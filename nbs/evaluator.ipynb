{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluator Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svelascodimate/miniconda3/envs/code-check-list/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import CodeCheckList\n",
    "import pandas as pd\n",
    "\n",
    "import CodeCheckList.utils as utils\n",
    "from CodeCheckList.tokenizer import CodeTokenizer\n",
    "from CodeCheckList.masker import Masker\n",
    "from CodeCheckList.predictor import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Evaluator:\n",
    "    \"\"\"Evaluator Module to perform all AST Evaluations\"\"\"\n",
    "    def __init__(self, checkpoint: str, language):\n",
    "        self.tokenizer = CodeTokenizer.from_pretrained(checkpoint, language)\n",
    "        self.masker = Masker(self.tokenizer)\n",
    "        self.predictor = Predictor.from_pretrained(checkpoint, self.tokenizer)\n",
    "\n",
    "    def __call__(self, test_set, number_of_predictions: int):\n",
    "        results_dict = self.evaluate_test_set(test_set, number_of_predictions)\n",
    "        results_dataframe = pd.DataFrame([], columns=['ast_element', 'occurences', 'successful_predictions', 'failed_predictions', 'total_predictions', 'success_average', 'failure_average'])\n",
    "        for result_index, result in enumerate(results_dict):\n",
    "            results_dataframe.loc[len(results_dataframe.index)] = [self.tokenizer.node_types[result_index] ,result[0], result[1], result[2], result[3], result[4], result[5]]\n",
    "        return results_dataframe\n",
    "    \n",
    "    def evaluate_test_set(self, test_set, number_of_predictions: int):\n",
    "        results_dict = []\n",
    "        for node_type in self.tokenizer.node_types:\n",
    "            results_dict.append([0, [0 for i in range(0,number_of_predictions)], \n",
    "                                [0 for i in range(0,number_of_predictions)], \n",
    "                                [0 for i in range(0,number_of_predictions)], \n",
    "                                [0 for i in range(0,number_of_predictions)],\n",
    "                                [0 for i in range(0,number_of_predictions)]])\n",
    "        for sample_index, sample in enumerate(test_set):\n",
    "            print('-------------evaluating sample:'+str(sample_index)+'---------------------')\n",
    "            sample_results = self.evaluate_code_snippet(sample['whole_func_string'], number_of_predictions)\n",
    "            for element_index, element_result in enumerate(sample_results):\n",
    "                element_result_values = list(element_result.values())[0]\n",
    "                if len(element_result_values) > 0:\n",
    "                    results_dict[element_index][0] += element_result_values[0][0]\n",
    "                    for prediction_number_index in range(0, number_of_predictions):\n",
    "                        results_dict[element_index][1][prediction_number_index]+= (1 if element_result_values[prediction_number_index][2] else 0)\n",
    "                        results_dict[element_index][2][prediction_number_index]+= (0 if element_result_values[prediction_number_index][2] else 1)\n",
    "                        results_dict[element_index][3][prediction_number_index]= results_dict[element_index][1][prediction_number_index] + results_dict[element_index][2][prediction_number_index]\n",
    "                        results_dict[element_index][4][prediction_number_index]= results_dict[element_index][1][prediction_number_index]/results_dict[element_index][3][prediction_number_index]\n",
    "                        results_dict[element_index][5][prediction_number_index]= results_dict[element_index][2][prediction_number_index]/results_dict[element_index][3][prediction_number_index]\n",
    "        return results_dict\n",
    "        \n",
    "    def evaluate_code_snippet(self, source_code: str, number_of_predictions: int):\n",
    "        evaluation_results = []\n",
    "        for node_type_idx, node_type in enumerate(self.tokenizer.node_types):\n",
    "            evaluation_results.append({node_type: self.evaluate_node_type_on_snippet(source_code, node_type_idx, number_of_predictions)})\n",
    "        return evaluation_results\n",
    "            \n",
    "    def evaluate_node_type_on_snippet(self, source_code: str, target_node_type_idx: int, number_of_predictions: int):\n",
    "        results=[]\n",
    "        source_code_nodes = []\n",
    "        utils.find_nodes(self.tokenizer.parser.parse(bytes(source_code, \"utf8\")).root_node, \n",
    "            self.tokenizer.node_types[target_node_type_idx], source_code_nodes)\n",
    "        if len(source_code_nodes) == 0:\n",
    "            return results\n",
    "\n",
    "        masked_code_encoding = self.masker(source_code, self.tokenizer(source_code), target_node_type_idx)\n",
    "        predictions = self.predictor(masked_code_encoding, self.tokenizer.tokenizer(source_code, return_tensors='pt'), number_of_predictions)  \n",
    "        for prediction_number in range(0, number_of_predictions):\n",
    "            predicted_code = predictions[prediction_number]\n",
    "            predicted_nodes = []\n",
    "            if utils.is_balanced_snippet(predicted_code):\n",
    "\n",
    "                predicted_code_tree = self.tokenizer.parser.parse(bytes(predicted_code, \"utf8\")).root_node\n",
    "                source_code_tree = self.tokenizer.parser.parse(bytes(source_code, \"utf8\")).root_node\n",
    "\n",
    "                predicted_code_types = utils.get_node_type_set(predicted_code_tree)\n",
    "                source_code_types = utils.get_node_type_set(source_code_tree)\n",
    "\n",
    "                jaccard_distance = utils.calculate_jaccard_distance(predicted_code_types, source_code_types)\n",
    "                sorensen_dice_distance = utils.calculate_sorensen_dice_distance(predicted_code_types, source_code_types)\n",
    "                \n",
    "            \n",
    "                utils.find_nodes(predicted_code_tree, self.tokenizer.node_types[target_node_type_idx], predicted_nodes)\n",
    "            results.append([len(source_code_nodes), len(predicted_nodes), len(predicted_nodes)>=len(source_code_nodes)])\n",
    "            \n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/svelascodimate/miniconda3/envs/code-check-list/lib/python3.10/site-packages/CodeCheckList/grammars\n"
     ]
    }
   ],
   "source": [
    "from CodeCheckList import loader\n",
    "\n",
    "\"\"\"define language\"\"\"\n",
    "python_language = \"python\"\n",
    "\n",
    "languages = [python_language]\n",
    "\n",
    "loader.download_grammars(languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"define the model checkpoint\"\"\"\n",
    "checkpoint = \"huggingface/CodeBERTa-small-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CodeCheckList.tokenizer import CodeTokenizer\n",
    "from CodeCheckList.masker import Masker\n",
    "\n",
    "#create code tokenizer \n",
    "bert_tokenizer = CodeTokenizer.from_pretrained(checkpoint, python_language)\n",
    "\n",
    "#create code masker\n",
    "code_masker = Masker(bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['import_statement', 'case_clause', 'elif', 'keyword_separator', 'yield', '>>=', '<', 'lambda_parameters', 'while', 'tuple_pattern', 'continue_statement', 'decorated_definition', 'dictionary_comprehension', '%', '>>', 'return', '~', 'import_from_statement', '|=', 'parameter', '@', ':=', '>', 'integer', 'in', 'default_parameter', 'finally', 'set', 'or', '}}', 'attribute', 'from', 'format_specifier', '*=', 'continue', 'assert', 'dictionary', '!=', 'format_expression', 'del', 'as', 'else', 'try', 'subscript', 'type_conversion', 'except', 'pair', ';', 'async', 'pass_statement', 'pass', '_simple_statement', 'break', '[', 'await', 'for_statement', ']', 'dictionary_splat', 'with_clause', '-', 'assignment', 'named_expression', '**=', 'match', '\"', '**', 'match_statement', '^=', 'float', 'typed_parameter', '==', 'comment', 'elif_clause', 'slice', 'typed_default_parameter', 'while_statement', '//=', 'type', '*', 'exec_statement', 'list_comprehension', 'try_statement', 'print_statement', ',', '+=', 'assert_statement', 'augmented_assignment', ':', 'future_import_statement', '<<', 'def', 'string', '&=', 'if', 'relative_import', 'decorator', 'ellipsis', 'raise_statement', 'keyword_argument', 'case_pattern', 'nonlocal', '.', 'argument_list', '__future__', 'with_statement', 'conditional_expression', '_compound_statement', 'true', 'break_statement', 'return_statement', '/=', 'generator_expression', ')', 'nonlocal_statement', 'as_pattern', 'print', '@=', '<>', '//', 'binary_operator', 'expression_list', 'parenthesized_list_splat', '(', 'lambda', 'dictionary_splat_pattern', 'list_pattern', 'list_splat_pattern', 'identifier', 'parameters', 'boolean_operator', 'positional_separator', '+', '->', 'parenthesized_expression', 'escape_sequence', 'not_operator', 'import_prefix', 'call', 'exec', 'else_clause', 'interpolation', 'false', 'not', 'none', 'is', 'chevron', 'list', 'list_splat', '-=', 'comparison_operator', '}', '<=', 'if_statement', 'concatenated_string', 'with_item', 'as_pattern_target', 'primary_expression', '>=', 'function_definition', 'global_statement', 'for_in_clause', 'ERROR', 'class_definition', '^', 'expression', 'aliased_import', '/', '%=', 'raise', 'pattern_list', 'tuple', '{', 'except_clause', 'if_clause', 'and', 'dotted_name', 'case', 'delete_statement', 'wildcard_import', '=', 'import', 'unary_operator', '<<=', 'block', 'module', '{{', 'class', '&', 'for', 'expression_statement', 'pattern', 'set_comprehension', '|', 'with', 'global', 'finally_clause']\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.node_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def multiply_numbers(a,b):\n",
      "    return a<mask>b\n"
     ]
    }
   ],
   "source": [
    "\"\"\"example source code\"\"\"\n",
    "\n",
    "code = \"def multiply_numbers(a,b):\\n    return a*b\"\n",
    "#code = \"def scale(self, center=True, scale=True):\\n        \\\"\\\"\\\"\\nthe the\\n\\n\\n                                                                                                                                                          _\\n                     ____________=_=_===========________===______________________________==_____________________\\n_______\\n____\\n\\n___\\n\\n\\n\\n\\n\\n\\n\\n\\n        return return)\"\n",
    "#code = \"def hello_world(a,b):\\n    print('hello world')\"\n",
    "#code = \"def __ordered_values_by_indexes(self, data, inds): \\\"\\\"\\\" Return values (intensities) by indexes. Used for multiscale graph cut. data = [[0 1 1], [0 2 2], [0 2 2]] inds = [[0 1 2], [3 4 4], [5 4 4]] return: [0, 1, 1, 0, 2, 0] If the data are not consistent, it will take the maximal value \\\"\\\"\\\" # get unique labels and their first indexes # lab, linds = np.unique(inds, return_index=True) # compute values by indexes # values = data.reshape(-1)[linds] # alternative slow implementation # if there are different data on same index, it will take # maximal value # lab = np.unique(inds) # values = [0]*len(lab) # for label in lab: # values[label] = np.max(data[inds == label]) # # values = np.asarray(values) # yet another implementation values = [None] * (np.max(inds) + 1) linear_inds = inds.ravel() linear_data = data.ravel() for i in range(0, len(linear_inds)): # going over all data pixels if values[linear_inds[i]] is None: # this index is found for first values[linear_inds[i]] = linear_data[i] elif values[linear_inds[i]] < linear_data[i]: # here can be changed maximal or minimal value values[linear_inds[i]] = linear_data[i] values = np.asarray(values) return values\"\n",
    "#code = \"def __ordered_values_by_indexes(self, data, inds):  # get unique labels and their first indexes # lab, linds = np.unique(inds, return_index=True) # compute values by indexes # values = data.reshape(-1)[linds] # alternative slow implementation # if there are different data on same index, it will take # maximal value # lab = np.unique(inds) # values = [0]*len(lab) # for label in lab: # values[label] = np.max(data[inds == label]) # # values = np.asarray(values) # yet another implementation values = [None] * (np.max(inds) + 1) linear_inds = inds.ravel() linear_data = data.ravel() for i in range(0, len(linear_inds)): # going over all data pixels if values[linear_inds[i]] is None: # this index is found for first values[linear_inds[i]] = linear_data[i] elif values[linear_inds[i]] < linear_data[i]: # here can be changed maximal or minimal value values[linear_inds[i]] = linear_data[i] values = np.asarray(values) return values\"\n",
    "target_node_type = \"*\"\n",
    "\n",
    "#encoding \n",
    "source_code_encoding = bert_tokenizer(code)\n",
    "\n",
    "#masking\n",
    "masked_code_encoding = code_masker(code, bert_tokenizer(code), bert_tokenizer.node_types.index(target_node_type))\n",
    "\n",
    "assert len(source_code_encoding['input_ids']) == len(masked_code_encoding['input_ids'])\n",
    "\n",
    "#masked code\n",
    "masked_code = bert_tokenizer.tokenizer.decode(list(filter(lambda token_id: False if token_id == bert_tokenizer.tokenizer.bos_token_id or \n",
    "            token_id == bert_tokenizer.tokenizer.eos_token_id else True, masked_code_encoding['input_ids'])))\n",
    "\n",
    "print(masked_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Loading Model into GPU------------------\n"
     ]
    }
   ],
   "source": [
    "from CodeCheckList.predictor import Predictor\n",
    "\n",
    "predictor = Predictor.from_pretrained(checkpoint, bert_tokenizer)\n",
    "predictions = predictor(masked_code_encoding, bert_tokenizer.tokenizer(code, return_tensors='pt'), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- CODE -------------\n",
      "def multiply_numbers(a,b):\n",
      "    return a*b\n",
      "\n",
      "---------- MASKED -------------\n",
      "def multiply_numbers(a,b):\n",
      "    return a<mask>b\n",
      "\n",
      "--------- PREDICTED -----------\n",
      "def multiply_numbers(a,b):\n",
      "    return a,b\n",
      "\n",
      "--------- AST COMPARE -----------\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import CodeCheckList.utils as utils\n",
    "\n",
    "prediction_number = 0\n",
    "print('------------- CODE -------------')\n",
    "print(code)\n",
    "print('\\n---------- MASKED -------------')\n",
    "print(masked_code)\n",
    "print('\\n--------- PREDICTED -----------')\n",
    "predicted_code = predictions[prediction_number]\n",
    "print(predicted_code)\n",
    "print('\\n--------- AST COMPARE -----------')\n",
    "filtered_nodes = []\n",
    "filtered_nodes_predict = []\n",
    "utils.find_nodes(bert_tokenizer.parser.parse(bytes(code, \"utf8\")).root_node, bert_tokenizer.node_types[bert_tokenizer.node_types.index(target_node_type)], filtered_nodes)\n",
    "utils.find_nodes(bert_tokenizer.parser.parse(bytes(predicted_code, \"utf8\")).root_node, bert_tokenizer.node_types[bert_tokenizer.node_types.index(target_node_type)], filtered_nodes_predict)\n",
    "print(len(filtered_nodes))\n",
    "print(len(filtered_nodes_predict))\n",
    "#base the evaluation on size comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Loading Model into GPU------------------\n",
      "510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: code_search_net/all\n",
      "Found cached dataset code_search_net (/home/svelascodimate/.cache/huggingface/datasets/code_search_net/all/1.0.0/80a244ab541c6b2125350b764dc5c2b715f65f00de7a56107a28915fac173a27)\n",
      "Parameter 'function'=<function <lambda> at 0x7fef041cd5a0> of the transform datasets.arrow_dataset.Dataset.filter@2.0.1 couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "  0%|          | 0/101 [00:00<?, ?ba/s]Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 101/101 [00:24<00:00,  4.05ba/s]\n",
      "No config specified, defaulting to: code_search_net/all\n",
      "Found cached dataset code_search_net (/home/svelascodimate/.cache/huggingface/datasets/code_search_net/all/1.0.0/80a244ab541c6b2125350b764dc5c2b715f65f00de7a56107a28915fac173a27)\n",
      "  0%|          | 0/101 [00:00<?, ?ba/s]Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 101/101 [00:25<00:00,  4.03ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "import CodeCheckList.utils as utils\n",
    "\n",
    "\n",
    "evaluator = Evaluator(checkpoint, python_language)\n",
    "\n",
    "max_token_number = bert_tokenizer.tokenizer.max_len_single_sentence\n",
    "print(max_token_number)\n",
    "\n",
    "test_set = load_dataset(\"code_search_net\", split='test')\n",
    "test_set = test_set.filter(lambda sample: True if sample['language']== python_language\n",
    "            and len(sample['func_code_tokens']) <= max_token_number\n",
    "            and len(bert_tokenizer.tokenizer(sample['whole_func_string'])['input_ids']) <= max_token_number else False, num_proc=1)\n",
    "\n",
    "test_set = utils.get_random_sub_set_test_set(utils.get_test_sets(load_dataset(\"code_search_net\", split='test'), \"python\", evaluator.tokenizer.tokenizer.max_len_single_sentence, evaluator.tokenizer), 2)\n",
    "\n",
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Loading Model into GPU------------------\n",
      "-------------evaluating sample:0---------------------\n",
      "0.0\n",
      "0.027777777777777776\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.027777777777777776\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.027777777777777776\n",
      "0.0\n",
      "0.0\n",
      "0.027777777777777776\n",
      "0.10256410256410256\n",
      "0.0\n",
      "0.0\n",
      "0.027777777777777776\n",
      "0.05405405405405406\n",
      "0.0\n",
      "0.0\n",
      "0.027777777777777776\n",
      "0.027777777777777776\n",
      "0.0\n",
      "0.05405405405405406\n",
      "0.0\n",
      "0.0\n",
      "0.027777777777777776\n",
      "0.027777777777777776\n",
      "0.0\n",
      "0.0\n",
      "0.027777777777777776\n",
      "0.07894736842105263\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.027777777777777776\n",
      "0.027777777777777776\n",
      "0.07894736842105263\n",
      "0.027777777777777776\n",
      "0.05405405405405406\n",
      "0.0\n",
      "0.0\n",
      "0.027777777777777776\n",
      "0.027777777777777776\n",
      "0.0\n",
      "0.10256410256410256\n",
      "0.07894736842105263\n",
      "0.027777777777777776\n",
      "0.0\n",
      "0.05405405405405406\n",
      "0.0\n",
      "0.027777777777777776\n",
      "0.07894736842105263\n",
      "0.027777777777777776\n",
      "0.0\n",
      "0.0\n",
      "0.05405405405405406\n",
      "0.125\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.05405405405405406\n",
      "0.027777777777777776\n",
      "0.05405405405405406\n",
      "0.05405405405405406\n",
      "0.125\n",
      "0.0\n",
      "0.10256410256410256\n",
      "0.10256410256410256\n",
      "0.0\n",
      "-------------evaluating sample:1---------------------\n",
      "0.13333333333333333\n",
      "0.11363636363636363\n",
      "0.025\n",
      "0.04878048780487805\n",
      "0.025\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ast_element</th>\n",
       "      <th>occurences</th>\n",
       "      <th>successful_predictions</th>\n",
       "      <th>failed_predictions</th>\n",
       "      <th>total_predictions</th>\n",
       "      <th>success_average</th>\n",
       "      <th>failure_average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>identifier</td>\n",
       "      <td>60</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>[0.5, 0.5]</td>\n",
       "      <td>[0.5, 0.5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>\"</td>\n",
       "      <td>18</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>[0.5, 0.0]</td>\n",
       "      <td>[0.5, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>.</td>\n",
       "      <td>16</td>\n",
       "      <td>[1, 0]</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>[0.5, 0.0]</td>\n",
       "      <td>[0.5, 1.0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>)</td>\n",
       "      <td>13</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>[0.5, 0.5]</td>\n",
       "      <td>[0.5, 0.5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>(</td>\n",
       "      <td>13</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[1, 1]</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>[0.5, 0.5]</td>\n",
       "      <td>[0.5, 0.5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>typed_default_parameter</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>while_statement</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>//=</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>type</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>finally_clause</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>[0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>196 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ast_element  occurences successful_predictions  \\\n",
       "127               identifier          60                 [1, 1]   \n",
       "64                         \"          18                 [1, 0]   \n",
       "101                        .          16                 [1, 0]   \n",
       "112                        )          13                 [1, 1]   \n",
       "122                        (          13                 [1, 1]   \n",
       "..                       ...         ...                    ...   \n",
       "74   typed_default_parameter           0                 [0, 0]   \n",
       "75           while_statement           0                 [0, 0]   \n",
       "76                       //=           0                 [0, 0]   \n",
       "77                      type           0                 [0, 0]   \n",
       "195           finally_clause           0                 [0, 0]   \n",
       "\n",
       "    failed_predictions total_predictions success_average failure_average  \n",
       "127             [1, 1]            [2, 2]      [0.5, 0.5]      [0.5, 0.5]  \n",
       "64              [1, 2]            [2, 2]      [0.5, 0.0]      [0.5, 1.0]  \n",
       "101             [1, 2]            [2, 2]      [0.5, 0.0]      [0.5, 1.0]  \n",
       "112             [1, 1]            [2, 2]      [0.5, 0.5]      [0.5, 0.5]  \n",
       "122             [1, 1]            [2, 2]      [0.5, 0.5]      [0.5, 0.5]  \n",
       "..                 ...               ...             ...             ...  \n",
       "74              [0, 0]            [0, 0]          [0, 0]          [0, 0]  \n",
       "75              [0, 0]            [0, 0]          [0, 0]          [0, 0]  \n",
       "76              [0, 0]            [0, 0]          [0, 0]          [0, 0]  \n",
       "77              [0, 0]            [0, 0]          [0, 0]          [0, 0]  \n",
       "195             [0, 0]            [0, 0]          [0, 0]          [0, 0]  \n",
       "\n",
       "[196 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_predictions = 2\n",
    "checkpoint = \"huggingface/CodeBERTa-small-v1\"\n",
    "python_language = \"python\"\n",
    "\n",
    "evaluator = Evaluator(checkpoint, python_language)\n",
    "\n",
    "results_dataframe = evaluator(test_set, number_of_predictions)\n",
    "\n",
    "results_dataframe.sort_values(by=['occurences'], ascending=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5dfa21b9ea908da29bde2e75ccf59a8bff4851a5152f1f941db0158f4a372e7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
