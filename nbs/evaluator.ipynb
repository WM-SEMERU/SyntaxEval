{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluator Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import CodeCheckList\n",
    "import pandas as pd\n",
    "\n",
    "import CodeCheckList.utils as utils\n",
    "from CodeCheckList.tokenizer import CodeTokenizer\n",
    "from CodeCheckList.masker import Masker\n",
    "from CodeCheckList.predictor import Predictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Evaluator:\n",
    "    \"\"\"Evaluator Module to perform all AST Evaluations\"\"\"\n",
    "    def __init__(self, checkpoint: str, language):\n",
    "        self.tokenizer = CodeTokenizer.from_pretrained(checkpoint, language)\n",
    "        self.masker = Masker(self.tokenizer)\n",
    "        self.predictor = Predictor.from_pretrained(checkpoint, self.tokenizer)\n",
    "\n",
    "    def __call__(self, test_set, number_of_predictions: int):\n",
    "        results_dict = self.evaluate_test_set(test_set, number_of_predictions)\n",
    "        results_dataframe = pd.DataFrame([], columns=['ast_element', 'occurences', 'successful_predictions', 'failed_predictions', 'total_predictions', 'success_average', 'failure_average'])\n",
    "        for result_index, result in enumerate(results_dict):\n",
    "            results_dataframe.loc[len(results_dataframe.index)] = [self.tokenizer.node_types[result_index] ,result[0], result[1], result[2], result[3], result[4], result[5]]\n",
    "        return results_dataframe\n",
    "    \n",
    "    def evaluate_test_set(self, test_set, number_of_predictions: int):\n",
    "        results_dict = []\n",
    "        for node_type in self.tokenizer.node_types:\n",
    "            results_dict.append([0, [0 for i in range(0,number_of_predictions)], \n",
    "                                [0 for i in range(0,number_of_predictions)], \n",
    "                                [0 for i in range(0,number_of_predictions)], \n",
    "                                [0 for i in range(0,number_of_predictions)],\n",
    "                                [0 for i in range(0,number_of_predictions)]])\n",
    "        for sample in test_set:\n",
    "            sample_results = self.evaluate_code_snippet(sample['whole_func_string'], number_of_predictions)\n",
    "            for element_index, element_result in enumerate(sample_results):\n",
    "                element_result_values = list(element_result.values())[0]\n",
    "                if len(element_result_values) > 0:\n",
    "                    results_dict[element_index][0] += element_result_values[0][0]\n",
    "                    for prediction_number_index in range(0, number_of_predictions):\n",
    "                        results_dict[element_index][1][prediction_number_index]+= (1 if element_result_values[prediction_number_index][2] else 0)\n",
    "                        results_dict[element_index][2][prediction_number_index]+= (0 if element_result_values[prediction_number_index][2] else 1)\n",
    "                        results_dict[element_index][3][prediction_number_index]= results_dict[element_index][1][prediction_number_index] + results_dict[element_index][2][prediction_number_index]\n",
    "                        results_dict[element_index][4][prediction_number_index]= results_dict[element_index][1][prediction_number_index]/results_dict[element_index][3][prediction_number_index]\n",
    "                        results_dict[element_index][5][prediction_number_index]= results_dict[element_index][2][prediction_number_index]/results_dict[element_index][3][prediction_number_index]\n",
    "        return results_dict\n",
    "        \n",
    "    def evaluate_code_snippet(self, source_code: str, number_of_predictions: int):\n",
    "        evaluation_results = []\n",
    "        for node_type_idx, node_type in enumerate(self.tokenizer.node_types):\n",
    "            evaluation_results.append({node_type: self.evaluate_node_type_on_snippet(source_code, node_type_idx, number_of_predictions)})\n",
    "        return evaluation_results\n",
    "            \n",
    "    def evaluate_node_type_on_snippet(self, source_code: str, target_node_type_idx: int, number_of_predictions: int):\n",
    "        results=[]\n",
    "        source_code_nodes = []\n",
    "        utils.find_nodes(self.tokenizer.parser.parse(bytes(source_code, \"utf8\")).root_node, \n",
    "            self.tokenizer.node_types[target_node_type_idx], source_code_nodes)\n",
    "        if len(source_code_nodes) == 0:\n",
    "            return results\n",
    "        #source_code_encoding = self.tokenizer(source_code)\n",
    "        masked_code_encoding = self.masker(source_code, self.tokenizer(source_code), target_node_type_idx)\n",
    "        #masked_code = self.tokenizer.tokenizer.decode(list(filter(lambda token_id: False if token_id == self.tokenizer.tokenizer.bos_token_id or \n",
    "        #    token_id == self.tokenizer.tokenizer.eos_token_id else True, masked_code_encoding['input_ids'])))\n",
    "        predictions = self.predictor(masked_code_encoding, self.tokenizer.tokenizer(source_code, return_tensors='pt'), number_of_predictions)  \n",
    "            \n",
    "        for prediction_number in range(0, number_of_predictions):\n",
    "            predicted_code = predictions[prediction_number]\n",
    "            predicted_nodes = []\n",
    "            utils.find_nodes(self.tokenizer.parser.parse(bytes(predicted_code, \"utf8\")).root_node, self.tokenizer.node_types[target_node_type_idx], predicted_nodes)\n",
    "            results.append([len(source_code_nodes), len(predicted_nodes), len(predicted_nodes)>=len(source_code_nodes)])\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/svelascodimate/miniconda3/envs/code-check-list/lib/python3.10/site-packages/CodeCheckList/grammars\n"
     ]
    }
   ],
   "source": [
    "from CodeCheckList import loader\n",
    "\n",
    "\"\"\"define language\"\"\"\n",
    "python_language = \"python\"\n",
    "\n",
    "languages = [python_language]\n",
    "\n",
    "loader.download_grammars(languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"define the model checkpoint\"\"\"\n",
    "checkpoint = \"huggingface/CodeBERTa-small-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CodeCheckList.tokenizer import CodeTokenizer\n",
    "from CodeCheckList.masker import Masker\n",
    "\n",
    "#create code tokenizer \n",
    "bert_tokenizer = CodeTokenizer.from_pretrained(checkpoint, python_language)\n",
    "\n",
    "#create code masker\n",
    "code_masker = Masker(bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['expression', 'expression_list', 'for', 'class_definition', 'lambda', 'if_clause', 'false', 'import_from_statement', '==', ')', 'break', 'import', 'if', '@=', 'async', 'and', 'with', '}', 'argument_list', 'block', 'pass', 'comparison_operator', 'print_statement', '(', 'concatenated_string', 'dictionary_splat_pattern', 'format_specifier', 'keyword_separator', 'function_definition', 'else_clause', 'elif_clause', 'wildcard_import', '%=', '^', 'finally', 'raise_statement', 'parenthesized_list_splat', 'list_comprehension', 'global', 'as_pattern', 'generator_expression', '~', '!=', 'exec_statement', '_compound_statement', 'expression_statement', '**', '&', 'break_statement', 'as_pattern_target', 'type', 'named_expression', 'relative_import', 'exec', 'not_operator', 'string', 'integer', 'def', 'try_statement', 'decorated_definition', 'assert', '|=', 'global_statement', 'list_splat_pattern', 'identifier', 'typed_parameter', 'case_pattern', 'continue', 'decorator', ']', 'escape_sequence', '{', 'binary_operator', 'lambda_parameters', '-=', 'format_expression', 'list', 'slice', 'import_statement', 'import_prefix', 'aliased_import', 'augmented_assignment', 'assignment', 'with_item', '+', 'tuple', '+=', '/', '%', 'pair', '&=', '<=', 'case', '>=', '//', 'list_splat', '=', 'keyword_argument', 'none', 'await', 'raise', 'conditional_expression', 'from', 'match_statement', 'for_statement', 'is', '**=', 'case_clause', 'positional_separator', 'match', 'dotted_name', 'if_statement', 'interpolation', 'comment', 'assert_statement', ';', '<<=', '__future__', 'nonlocal_statement', 'dictionary_splat', 'module', 'except_clause', '/=', '<', 'print', 'subscript', '{{', 'parameters', 'elif', 'yield', 'chevron', '<<', 'while', ':=', 'with_clause', 'true', 'primary_expression', 'finally_clause', '-', 'unary_operator', 'ellipsis', 'boolean_operator', 'nonlocal', 'try', 'parameter', 'dictionary_comprehension', 'future_import_statement', 'for_in_clause', 'else', 'or', 'del', '->', ':', 'typed_default_parameter', 'set', '>', 'delete_statement', 'set_comprehension', 'float', 'except', 'tuple_pattern', '}}', 'call', 'not', 'pattern_list', 'as', 'while_statement', 'list_pattern', '*=', 'in', 'type_conversion', '.', '>>', '>>=', 'class', 'return', 'continue_statement', '^=', 'return_statement', '_simple_statement', 'dictionary', '@', 'default_parameter', 'attribute', 'with_statement', 'pass_statement', 'parenthesized_expression', '//=', '<>', ',', '[', '*', '\"', 'pattern', '|']\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.node_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def multiply_numbers(a,b):\n",
      "    return a<mask>b\n"
     ]
    }
   ],
   "source": [
    "\"\"\"example source code\"\"\"\n",
    "\n",
    "code = \"def multiply_numbers(a,b):\\n    return a*b\"\n",
    "#code = \"def hello_world(a,b):\\n    print('hello world')\"\n",
    "#code = \"def __ordered_values_by_indexes(self, data, inds): \\\"\\\"\\\" Return values (intensities) by indexes. Used for multiscale graph cut. data = [[0 1 1], [0 2 2], [0 2 2]] inds = [[0 1 2], [3 4 4], [5 4 4]] return: [0, 1, 1, 0, 2, 0] If the data are not consistent, it will take the maximal value \\\"\\\"\\\" # get unique labels and their first indexes # lab, linds = np.unique(inds, return_index=True) # compute values by indexes # values = data.reshape(-1)[linds] # alternative slow implementation # if there are different data on same index, it will take # maximal value # lab = np.unique(inds) # values = [0]*len(lab) # for label in lab: # values[label] = np.max(data[inds == label]) # # values = np.asarray(values) # yet another implementation values = [None] * (np.max(inds) + 1) linear_inds = inds.ravel() linear_data = data.ravel() for i in range(0, len(linear_inds)): # going over all data pixels if values[linear_inds[i]] is None: # this index is found for first values[linear_inds[i]] = linear_data[i] elif values[linear_inds[i]] < linear_data[i]: # here can be changed maximal or minimal value values[linear_inds[i]] = linear_data[i] values = np.asarray(values) return values\"\n",
    "#code = \"def __ordered_values_by_indexes(self, data, inds):  # get unique labels and their first indexes # lab, linds = np.unique(inds, return_index=True) # compute values by indexes # values = data.reshape(-1)[linds] # alternative slow implementation # if there are different data on same index, it will take # maximal value # lab = np.unique(inds) # values = [0]*len(lab) # for label in lab: # values[label] = np.max(data[inds == label]) # # values = np.asarray(values) # yet another implementation values = [None] * (np.max(inds) + 1) linear_inds = inds.ravel() linear_data = data.ravel() for i in range(0, len(linear_inds)): # going over all data pixels if values[linear_inds[i]] is None: # this index is found for first values[linear_inds[i]] = linear_data[i] elif values[linear_inds[i]] < linear_data[i]: # here can be changed maximal or minimal value values[linear_inds[i]] = linear_data[i] values = np.asarray(values) return values\"\n",
    "target_node_type = \"*\"\n",
    "\n",
    "#encoding \n",
    "source_code_encoding = bert_tokenizer(code)\n",
    "\n",
    "#masking\n",
    "masked_code_encoding = code_masker(code, bert_tokenizer(code), bert_tokenizer.node_types.index(target_node_type))\n",
    "\n",
    "assert len(source_code_encoding['input_ids']) == len(masked_code_encoding['input_ids'])\n",
    "\n",
    "#masked code\n",
    "masked_code = bert_tokenizer.tokenizer.decode(list(filter(lambda token_id: False if token_id == bert_tokenizer.tokenizer.bos_token_id or \n",
    "            token_id == bert_tokenizer.tokenizer.eos_token_id else True, masked_code_encoding['input_ids'])))\n",
    "\n",
    "print(masked_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CodeCheckList.predictor import Predictor\n",
    "\n",
    "predictor = Predictor.from_pretrained(checkpoint, bert_tokenizer)\n",
    "predictions = predictor(masked_code_encoding, bert_tokenizer.tokenizer(code, return_tensors='pt'), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- CODE -------------\n",
      "def multiply_numbers(a,b):\n",
      "    return a*b\n",
      "\n",
      "---------- MASKED -------------\n",
      "def multiply_numbers(a,b):\n",
      "    return a<mask>b\n",
      "\n",
      "--------- PREDICTED -----------\n",
      "def multiply_numbers(a,b):\n",
      "    return a,b\n",
      "\n",
      "--------- AST COMPARE -----------\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import CodeCheckList.utils as utils\n",
    "\n",
    "prediction_number = 0\n",
    "print('------------- CODE -------------')\n",
    "print(code)\n",
    "print('\\n---------- MASKED -------------')\n",
    "print(masked_code)\n",
    "print('\\n--------- PREDICTED -----------')\n",
    "predicted_code = predictions[prediction_number]\n",
    "print(predicted_code)\n",
    "print('\\n--------- AST COMPARE -----------')\n",
    "filtered_nodes = []\n",
    "filtered_nodes_predict = []\n",
    "utils.find_nodes(bert_tokenizer.parser.parse(bytes(code, \"utf8\")).root_node, bert_tokenizer.node_types[bert_tokenizer.node_types.index(target_node_type)], filtered_nodes)\n",
    "utils.find_nodes(bert_tokenizer.parser.parse(bytes(predicted_code, \"utf8\")).root_node, bert_tokenizer.node_types[bert_tokenizer.node_types.index(target_node_type)], filtered_nodes_predict)\n",
    "print(len(filtered_nodes))\n",
    "print(len(filtered_nodes_predict))\n",
    "#base the evaluation on size comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'expression': []}, {'expression_list': []}, {'for': []}, {'class_definition': []}, {'lambda': []}, {'if_clause': []}, {'false': []}, {'import_from_statement': []}, {'==': []}, {')': [[1, 1, True], [1, 1, True], [1, 1, True], [1, 1, True], [1, 1, True]]}, {'break': []}, {'import': []}, {'if': []}, {'@=': []}, {'async': []}, {'and': []}, {'with': []}, {'}': []}, {'argument_list': []}, {'block': [[1, 1, True], [1, 1, True], [1, 1, True], [1, 1, True], [1, 1, True]]}, {'pass': []}, {'comparison_operator': []}, {'print_statement': []}, {'(': [[1, 1, True], [1, 1, True], [1, 1, True], [1, 1, True], [1, 1, True]]}, {'concatenated_string': []}, {'dictionary_splat_pattern': []}, {'format_specifier': []}, {'keyword_separator': []}, {'function_definition': [[1, 0, False], [1, 0, False], [1, 0, False], [1, 0, False], [1, 0, False]]}, {'else_clause': []}, {'elif_clause': []}, {'wildcard_import': []}, {'%=': []}, {'^': []}, {'finally': []}, {'raise_statement': []}, {'parenthesized_list_splat': []}, {'list_comprehension': []}, {'global': []}, {'as_pattern': []}, {'generator_expression': []}, {'~': []}, {'!=': []}, {'exec_statement': []}, {'_compound_statement': []}, {'expression_statement': []}, {'**': []}, {'&': []}, {'break_statement': []}, {'as_pattern_target': []}, {'type': []}, {'named_expression': []}, {'relative_import': []}, {'exec': []}, {'not_operator': []}, {'string': []}, {'integer': []}, {'def': [[1, 1, True], [1, 0, False], [1, 1, True], [1, 0, False], [1, 0, False]]}, {'try_statement': []}, {'decorated_definition': []}, {'assert': []}, {'|=': []}, {'global_statement': []}, {'list_splat_pattern': []}, {'identifier': [[5, 5, True], [5, 5, True], [5, 6, True], [5, 5, True], [5, 6, True]]}, {'typed_parameter': []}, {'case_pattern': []}, {'continue': []}, {'decorator': []}, {']': []}, {'escape_sequence': []}, {'{': []}, {'binary_operator': [[1, 0, False], [1, 0, False], [1, 1, True], [1, 0, False], [1, 0, False]]}, {'lambda_parameters': []}, {'-=': []}, {'format_expression': []}, {'list': []}, {'slice': []}, {'import_statement': []}, {'import_prefix': []}, {'aliased_import': []}, {'augmented_assignment': []}, {'assignment': []}, {'with_item': []}, {'+': []}, {'tuple': []}, {'+=': []}, {'/': []}, {'%': []}, {'pair': []}, {'&=': []}, {'<=': []}, {'case': []}, {'>=': []}, {'//': []}, {'list_splat': []}, {'=': []}, {'keyword_argument': []}, {'none': []}, {'await': []}, {'raise': []}, {'conditional_expression': []}, {'from': []}, {'match_statement': []}, {'for_statement': []}, {'is': []}, {'**=': []}, {'case_clause': []}, {'positional_separator': []}, {'match': []}, {'dotted_name': []}, {'if_statement': []}, {'interpolation': []}, {'comment': []}, {'assert_statement': []}, {';': []}, {'<<=': []}, {'__future__': []}, {'nonlocal_statement': []}, {'dictionary_splat': []}, {'module': [[1, 1, True], [1, 1, True], [1, 1, True], [1, 1, True], [1, 1, True]]}, {'except_clause': []}, {'/=': []}, {'<': []}, {'print': []}, {'subscript': []}, {'{{': []}, {'parameters': [[1, 1, True], [1, 1, True], [1, 0, False], [1, 1, True], [1, 0, False]]}, {'elif': []}, {'yield': []}, {'chevron': []}, {'<<': []}, {'while': []}, {':=': []}, {'with_clause': []}, {'true': []}, {'primary_expression': []}, {'finally_clause': []}, {'-': []}, {'unary_operator': []}, {'ellipsis': []}, {'boolean_operator': []}, {'nonlocal': []}, {'try': []}, {'parameter': []}, {'dictionary_comprehension': []}, {'future_import_statement': []}, {'for_in_clause': []}, {'else': []}, {'or': []}, {'del': []}, {'->': []}, {':': [[1, 1, True], [1, 1, True], [1, 1, True], [1, 1, True], [1, 1, True]]}, {'typed_default_parameter': []}, {'set': []}, {'>': []}, {'delete_statement': []}, {'set_comprehension': []}, {'float': []}, {'except': []}, {'tuple_pattern': []}, {'}}': []}, {'call': []}, {'not': []}, {'pattern_list': []}, {'as': []}, {'while_statement': []}, {'list_pattern': []}, {'*=': []}, {'in': []}, {'type_conversion': []}, {'.': []}, {'>>': []}, {'>>=': []}, {'class': []}, {'return': [[1, 1, True], [1, 0, False], [1, 0, False], [1, 0, False], [1, 0, False]]}, {'continue_statement': []}, {'^=': []}, {'return_statement': [[1, 1, True], [1, 1, True], [1, 1, True], [1, 0, False], [1, 0, False]]}, {'_simple_statement': []}, {'dictionary': []}, {'@': []}, {'default_parameter': []}, {'attribute': []}, {'with_statement': []}, {'pass_statement': []}, {'parenthesized_expression': []}, {'//=': []}, {'<>': []}, {',': [[1, 1, True], [1, 0, False], [1, 0, False], [1, 1, True], [1, 1, True]]}, {'[': []}, {'*': [[1, 0, False], [1, 1, True], [1, 0, False], [1, 0, False], [1, 0, False]]}, {'\"': []}, {'pattern': []}, {'|': []}]\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(checkpoint, python_language)\n",
    "\n",
    "results = evaluator.evaluate_code_snippet(code, 5)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: code_search_net/all\n",
      "Found cached dataset code_search_net (/home/svelascodimate/.cache/huggingface/datasets/code_search_net/all/1.0.0/80a244ab541c6b2125350b764dc5c2b715f65f00de7a56107a28915fac173a27)\n",
      "  0%|          | 0/101 [00:00<?, ?ba/s]Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 101/101 [00:30<00:00,  3.28ba/s]\n",
      "No config specified, defaulting to: code_search_net/all\n",
      "Found cached dataset code_search_net (/home/svelascodimate/.cache/huggingface/datasets/code_search_net/all/1.0.0/80a244ab541c6b2125350b764dc5c2b715f65f00de7a56107a28915fac173a27)\n",
      "  0%|          | 0/101 [00:00<?, ?ba/s]Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 101/101 [00:32<00:00,  3.11ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "import CodeCheckList.utils as utils\n",
    "\n",
    "max_token_number = bert_tokenizer.tokenizer.max_len_single_sentence\n",
    "print(max_token_number)\n",
    "\n",
    "test_set = load_dataset(\"code_search_net\", split='test')\n",
    "test_set = test_set.filter(lambda sample: True if sample['language']== python_language\n",
    "            and len(sample['func_code_tokens']) <= max_token_number\n",
    "            and len(bert_tokenizer.tokenizer(sample['whole_func_string'])['input_ids']) <= max_token_number else False, num_proc=1)\n",
    "\n",
    "test_set = utils.get_random_sub_set_test_set(utils.get_test_sets(load_dataset(\"code_search_net\", split='test'), \"python\", evaluator.tokenizer.tokenizer.max_len_single_sentence, evaluator.tokenizer), 2)\n",
    "\n",
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_predictions = 2\n",
    "checkpoint = \"huggingface/CodeBERTa-small-v1\"\n",
    "python_language = \"python\"\n",
    "\n",
    "evaluator = Evaluator(checkpoint, python_language)\n",
    "\n",
    "results_dataframe = evaluator(test_set, number_of_predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5dfa21b9ea908da29bde2e75ccf59a8bff4851a5152f1f941db0158f4a372e7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
