{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/svelascodimate/miniconda3/envs/code-check-list/lib/python3.10/site-packages/CodeCheckList/grammars\n"
     ]
    }
   ],
   "source": [
    "from CodeCheckList import loader\n",
    "\n",
    "\"\"\"define language\"\"\"\n",
    "python_language = \"python\"\n",
    "\n",
    "languages = [python_language]\n",
    "\n",
    "loader.download_grammars(languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"define the model checkpoint\"\"\"\n",
    "checkpoint = \"huggingface/CodeBERTa-small-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CodeCheckList.tokenizer import CodeTokenizer\n",
    "from CodeCheckList.masker import Masker\n",
    "\n",
    "#create code tokenizer \n",
    "bert_tokenizer = CodeTokenizer.from_pretrained(checkpoint, python_language)\n",
    "\n",
    "#create code masker\n",
    "code_masker = Masker(bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yield', '%', ':=', 'lambda', 'expression', 'float', '^', '>>=', '^=', 'case', 'default_parameter', 'or', '~', '->', '&=', 'decorator', 'in', '_compound_statement', '+', 'while', 'global_statement', 'try_statement', ':', 'for', 'list_pattern', '@=', 'with', 'as', 'parameter', '.', 'finally_clause', '<', '-', '{', 'del', 'block', 'await', '//=', 'exec', 'set_comprehension', 'try', 'pair', 'subscript', 'conditional_expression', 'if_statement', '>=', 'class_definition', 'elif_clause', '(', 'global', 'pass', 'string', 'delete_statement', 'slice', 'raise_statement', 'comment', 'false', 'wildcard_import', 'none', 'tuple', 'aliased_import', ';', 'assert_statement', 'else', '}}', 'augmented_assignment', 'nonlocal_statement', 'keyword_argument', 'for_in_clause', 'format_specifier', 'ellipsis', 'type_conversion', '=', '**=', 'module', 'typed_default_parameter', '!=', 'match_statement', 'dictionary_comprehension', 'expression_statement', '|=', 'true', 'return', '>>', '}', 'attribute', 'assignment', 'except_clause', '@', 'import_statement', '<=', 'type', ',', 'list_splat', 'integer', 'concatenated_string', 'else_clause', 'break_statement', '{{', 'pattern', 'dotted_name', 'print_statement', '<>', 'continue_statement', 'chevron', 'exec_statement', 'dictionary', 'boolean_operator', '+=', 'expression_list', 'raise', '_simple_statement', 'list_comprehension', 'unary_operator', 'parenthesized_list_splat', '==', 'import_prefix', '|', 'tuple_pattern', 'nonlocal', '&', 'def', 'list', 'pattern_list', 'future_import_statement', 'as_pattern', 'with_item', 'dictionary_splat', 'set', '**', 'comparison_operator', 'import', 'keyword_separator', '%=', 'break', 'interpolation', 'return_statement', 'decorated_definition', '//', '/=', 'call', 'for_statement', 'continue', 'and', 'while_statement', 'identifier', 'print', 'typed_parameter', 'case_pattern', 'elif', 'binary_operator', 'relative_import', 'parameters', 'escape_sequence', 'if', 'match', '<<', 'assert', 'class', 'parenthesized_expression', 'except', '\"', 'list_splat_pattern', 'async', 'case_clause', 'positional_separator', 'from', 'pass_statement', 'not', '-=', 'with_clause', '/', 'dictionary_splat_pattern', '<<=', '*=', 'format_expression', 'if_clause', ']', 'argument_list', ')', 'finally', 'not_operator', 'generator_expression', '__future__', 'function_definition', 'import_from_statement', 'with_statement', 'is', '*', 'primary_expression', 'lambda_parameters', '[', '>', 'named_expression']\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.node_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def multiply_numbers(a,b):\n",
      "    return a<mask>b\n"
     ]
    }
   ],
   "source": [
    "\"\"\"example source code\"\"\"\n",
    "\n",
    "code = \"def multiply_numbers(a,b):\\n    return a*b\"\n",
    "target_node_type = \"*\"\n",
    "\n",
    "#encoding \n",
    "source_code_encoding = bert_tokenizer(code)\n",
    "\n",
    "#masking\n",
    "masked_code_encoding = code_masker(code, bert_tokenizer(code), bert_tokenizer.node_types.index(target_node_type))\n",
    "\n",
    "assert len(source_code_encoding['input_ids']) == len(masked_code_encoding['input_ids'])\n",
    "\n",
    "#masked code\n",
    "masked_code = bert_tokenizer.tokenizer.decode(list(filter(lambda token_id: False if token_id == bert_tokenizer.tokenizer.bos_token_id or \n",
    "            token_id == bert_tokenizer.tokenizer.eos_token_id else True, masked_code_encoding['input_ids'])))\n",
    "\n",
    "print(masked_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([14.7716, 14.7396], grad_fn=<TopkBackward0>)\n",
      ",*\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
    "\n",
    "masked_indexes = list(map(lambda entry: entry[0],\n",
    "    list(filter(lambda entry: True if entry[1] == bert_tokenizer.tokenizer.mask_token_id else False, enumerate(masked_code_encoding['input_ids'])))))\n",
    "\n",
    "code_encoding = bert_tokenizer.tokenizer(code, return_tensors='pt')\n",
    "code_encoding['input_ids'][0] = torch.tensor([torch.tensor(input_id) for input_id in masked_code_encoding['input_ids']])\n",
    "\n",
    "model_prediction = model(**code_encoding)\n",
    "\n",
    "for masked_index in masked_indexes:\n",
    "    values, predictions = model_prediction['logits'][0][masked_index].topk(2)\n",
    "    print(values)\n",
    "    print(bert_tokenizer.tokenizer.decode(predictions))\n",
    "    code_encoding['input_ids'][0][masked_index] = predictions[0]\n",
    "\n",
    "predicted_code = bert_tokenizer.tokenizer.decode(code_encoding['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def multiply_numbers(a,b):\n",
      "    return a*b\n",
      "def multiply_numbers(a,b):\n",
      "    return a<mask>b\n",
      "<s>def multiply_numbers(a,b):\n",
      "    return a,b</s>\n"
     ]
    }
   ],
   "source": [
    "print(code)\n",
    "print(masked_code)\n",
    "print(predicted_code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5dfa21b9ea908da29bde2e75ccf59a8bff4851a5152f1f941db0158f4a372e7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
