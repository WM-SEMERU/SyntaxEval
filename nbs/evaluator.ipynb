{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/svelascodimate/miniconda3/envs/code-check-list/lib/python3.10/site-packages/CodeCheckList/grammars\n"
     ]
    }
   ],
   "source": [
    "from CodeCheckList import loader\n",
    "\n",
    "\"\"\"define language\"\"\"\n",
    "python_language = \"python\"\n",
    "\n",
    "languages = [python_language]\n",
    "\n",
    "loader.download_grammars(languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"define the model checkpoint\"\"\"\n",
    "checkpoint = \"huggingface/CodeBERTa-small-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CodeCheckList.tokenizer import CodeTokenizer\n",
    "from CodeCheckList.masker import Masker\n",
    "\n",
    "#create code tokenizer \n",
    "bert_tokenizer = CodeTokenizer.from_pretrained(checkpoint, python_language)\n",
    "\n",
    "#create code masker\n",
    "code_masker = Masker(bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['>=', 'unary_operator', 'chevron', 'return_statement', 'pair', 'break', 'lambda_parameters', 'exec', 'case_clause', 'false', 'list', 'global', 'format_expression', 'import', '<>', 'block', 'tuple', 'except', 'for', '[', 'pass', 'format_specifier', '^', 'for_statement', 'typed_default_parameter', 'with_statement', 'case', 'assignment', '<<=', 'assert', 'list_splat_pattern', 'and', '<<', 'if_clause', '//', '%=', '(', 'exec_statement', 'ellipsis', 'if_statement', 'continue_statement', 'keyword_separator', ':', 'parenthesized_list_splat', 'finally', ',', 'nonlocal', 'binary_operator', 'module', 'try_statement', 'conditional_expression', 'with_clause', 'as_pattern', 'import_prefix', '/=', 'else', '__future__', 'list_pattern', '@', 'await', 'import_from_statement', '&=', ';', 'expression_list', 'is', '}', 'not', '->', 'augmented_assignment', 'set', 'wildcard_import', '/', '>', 'async', 'if', '^=', '<', 'or', 'interpolation', 'generator_expression', 'decorated_definition', '**', '**=', 'from', 'not_operator', 'lambda', 'match', 'raise', 'future_import_statement', '*', 'parameter', 'boolean_operator', '>>', 'slice', 'class_definition', '+=', '-=', 'dotted_name', 'elif_clause', 'float', '//=', 'parenthesized_expression', '>>=', 'expression', 'with', '-', '{', 'global_statement', 'identifier', 'named_expression', '=', 'for_in_clause', 'delete_statement', 'print_statement', 'raise_statement', '{{', 'decorator', 'concatenated_string', 'true', 'break_statement', 'call', 'escape_sequence', 'elif', '%', 'in', 'none', 'del', 'dictionary_comprehension', 'def', 'yield', 'tuple_pattern', 'assert_statement', '==', 'comparison_operator', 'print', 'type', 'keyword_argument', 'expression_statement', 'return', 'match_statement', 'set_comprehension', 'case_pattern', '*=', 'string', 'argument_list', 'positional_separator', 'pass_statement', 'with_item', '|=', 'else_clause', 'finally_clause', '|', 'aliased_import', 'relative_import', ':=', 'integer', '}}', '.', 'dictionary_splat', 'list_splat', 'import_statement', 'while_statement', '_compound_statement', '_simple_statement', 'subscript', ')', '@=', '\"', '+', ']', 'dictionary_splat_pattern', 'comment', 'except_clause', 'default_parameter', 'pattern', '&', '<=', 'while', 'typed_parameter', 'as', 'nonlocal_statement', 'attribute', 'function_definition', 'try', 'dictionary', 'pattern_list', 'continue', 'parameters', 'type_conversion', 'list_comprehension', 'primary_expression', '!=', 'class', '~']\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.node_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def hello_world(a,b):\n",
      "    print('<mask><mask>')\n"
     ]
    }
   ],
   "source": [
    "\"\"\"example source code\"\"\"\n",
    "\n",
    "code = \"def multiply_numbers(a,b):\\n    return a*b\"\n",
    "code = \"def hello_world(a,b):\\n    print('hello world')\"\n",
    "target_node_type = \"string\"\n",
    "\n",
    "#encoding \n",
    "source_code_encoding = bert_tokenizer(code)\n",
    "\n",
    "#masking\n",
    "masked_code_encoding = code_masker(code, bert_tokenizer(code), bert_tokenizer.node_types.index(target_node_type))\n",
    "\n",
    "assert len(source_code_encoding['input_ids']) == len(masked_code_encoding['input_ids'])\n",
    "\n",
    "#masked code\n",
    "masked_code = bert_tokenizer.tokenizer.decode(list(filter(lambda token_id: False if token_id == bert_tokenizer.tokenizer.bos_token_id or \n",
    "            token_id == bert_tokenizer.tokenizer.eos_token_id else True, masked_code_encoding['input_ids'])))\n",
    "\n",
    "print(masked_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CodeCheckList.predictor import Predictor\n",
    "\n",
    "predictor = Predictor.from_pretrained(checkpoint, bert_tokenizer)\n",
    "predictions = predictor(masked_code_encoding, bert_tokenizer.tokenizer(code, return_tensors='pt'), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def hello_world(a,b):\n",
      "    print('hello world')\n",
      "def hello_world(a,b):\n",
      "    print('<mask><mask>')\n",
      "def hello_world(a,b):\n",
      "    print('Hello world')\n"
     ]
    }
   ],
   "source": [
    "print(code)\n",
    "print(masked_code)\n",
    "print(predictions[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5dfa21b9ea908da29bde2e75ccf59a8bff4851a5152f1f941db0158f4a372e7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
