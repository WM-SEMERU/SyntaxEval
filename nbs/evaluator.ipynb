{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/svelascodimate/miniconda3/envs/code-check-list/lib/python3.10/site-packages/CodeCheckList/grammars\n"
     ]
    }
   ],
   "source": [
    "from CodeCheckList import loader\n",
    "\n",
    "\"\"\"define language\"\"\"\n",
    "python_language = \"python\"\n",
    "\n",
    "languages = [python_language]\n",
    "\n",
    "loader.download_grammars(languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"define the model checkpoint\"\"\"\n",
    "checkpoint = \"huggingface/CodeBERTa-small-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svelascodimate/miniconda3/envs/code-check-list/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from CodeCheckList.tokenizer import CodeTokenizer\n",
    "from CodeCheckList.masker import Masker\n",
    "\n",
    "#create code tokenizer \n",
    "bert_tokenizer = CodeTokenizer.from_pretrained(checkpoint, python_language)\n",
    "\n",
    "#create code masker\n",
    "code_masker = Masker(bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ellipsis', 'pattern', 'exec', 'pass', 'call', 'expression_statement', 'assignment', 'as', 'dictionary_comprehension', 'block', 'expression', 'format_expression', 'while_statement', '/', 'argument_list', ':', 'case_pattern', 'from', '&', '!=', 'print_statement', 'type_conversion', '_simple_statement', 'async', '-', 'keyword_argument', 'keyword_separator', 'positional_separator', 'break_statement', 'match_statement', 'unary_operator', 'except', '<<', '**=', '<', 'import', 'exec_statement', '^=', '@', 'except_clause', 'dictionary_splat_pattern', 'continue', 'concatenated_string', 'is', 'parameter', 'import_prefix', 'list_splat_pattern', 'pair', 'class', 'yield', 'return', '-=', 'format_specifier', '=', 'set', 'binary_operator', 'list_splat', 'else', '_compound_statement', 'case', 'typed_default_parameter', 'delete_statement', 'lambda_parameters', '>>=', 'print', 'relative_import', 'subscript', 'primary_expression', '{', 'boolean_operator', 'case_clause', 'string', 'escape_sequence', 'import_from_statement', 'expression_list', ',', '->', 'break', 'elif_clause', 'comment', 'dictionary', 'aliased_import', 'interpolation', 'finally', 'lambda', 'pass_statement', '{{', 'chevron', 'continue_statement', 'raise_statement', 'parenthesized_expression', 'augmented_assignment', '<=', 'list', 'try_statement', 'integer', 'typed_parameter', 'else_clause', 'attribute', '|', 'finally_clause', 'elif', '==', 'not_operator', 'and', 'future_import_statement', 'if', 'in', 'pattern_list', 'assert_statement', '/=', 'module', 'list_comprehension', '&=', '__future__', 'list_pattern', 'return_statement', '[', '//=', ')', 'import_statement', ':=', 'or', '}', '//', 'with_statement', '>>', '<<=', 'generator_expression', '+=', 'nonlocal_statement', 'await', 'decorator', '**', 'raise', 'dictionary_splat', 'named_expression', 'parameters', ']', '@=', 'decorated_definition', 'for', '%=', '+', 'conditional_expression', ';', '*', 'tuple_pattern', 'default_parameter', '|=', 'class_definition', 'identifier', 'for_in_clause', 'dotted_name', 'with_item', '.', '*=', '\"', 'false', '>=', 'try', 'if_statement', 'if_clause', 'slice', 'del', 'def', 'function_definition', 'comparison_operator', 'tuple', 'parenthesized_list_splat', 'global', 'with', '<>', 'global_statement', '~', 'set_comprehension', '(', 'type', 'wildcard_import', 'not', '%', 'as_pattern', 'match', 'while', '}}', 'with_clause', 'assert', 'nonlocal', '^', 'true', 'float', 'none', 'for_statement', '>']\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.node_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def multiply_numbers(a,b):\n",
      "    return a<mask>b\n"
     ]
    }
   ],
   "source": [
    "\"\"\"example source code\"\"\"\n",
    "\n",
    "code = \"def multiply_numbers(a,b):\\n    return a*b\"\n",
    "#code = \"def hello_world(a,b):\\n    print('hello world')\"\n",
    "#code = \"def __ordered_values_by_indexes(self, data, inds): \\\"\\\"\\\" Return values (intensities) by indexes. Used for multiscale graph cut. data = [[0 1 1], [0 2 2], [0 2 2]] inds = [[0 1 2], [3 4 4], [5 4 4]] return: [0, 1, 1, 0, 2, 0] If the data are not consistent, it will take the maximal value \\\"\\\"\\\" # get unique labels and their first indexes # lab, linds = np.unique(inds, return_index=True) # compute values by indexes # values = data.reshape(-1)[linds] # alternative slow implementation # if there are different data on same index, it will take # maximal value # lab = np.unique(inds) # values = [0]*len(lab) # for label in lab: # values[label] = np.max(data[inds == label]) # # values = np.asarray(values) # yet another implementation values = [None] * (np.max(inds) + 1) linear_inds = inds.ravel() linear_data = data.ravel() for i in range(0, len(linear_inds)): # going over all data pixels if values[linear_inds[i]] is None: # this index is found for first values[linear_inds[i]] = linear_data[i] elif values[linear_inds[i]] < linear_data[i]: # here can be changed maximal or minimal value values[linear_inds[i]] = linear_data[i] values = np.asarray(values) return values\"\n",
    "#code = \"def __ordered_values_by_indexes(self, data, inds):  # get unique labels and their first indexes # lab, linds = np.unique(inds, return_index=True) # compute values by indexes # values = data.reshape(-1)[linds] # alternative slow implementation # if there are different data on same index, it will take # maximal value # lab = np.unique(inds) # values = [0]*len(lab) # for label in lab: # values[label] = np.max(data[inds == label]) # # values = np.asarray(values) # yet another implementation values = [None] * (np.max(inds) + 1) linear_inds = inds.ravel() linear_data = data.ravel() for i in range(0, len(linear_inds)): # going over all data pixels if values[linear_inds[i]] is None: # this index is found for first values[linear_inds[i]] = linear_data[i] elif values[linear_inds[i]] < linear_data[i]: # here can be changed maximal or minimal value values[linear_inds[i]] = linear_data[i] values = np.asarray(values) return values\"\n",
    "target_node_type = \"*\"\n",
    "\n",
    "#encoding \n",
    "source_code_encoding = bert_tokenizer(code)\n",
    "\n",
    "#masking\n",
    "masked_code_encoding = code_masker(code, bert_tokenizer(code), bert_tokenizer.node_types.index(target_node_type))\n",
    "\n",
    "assert len(source_code_encoding['input_ids']) == len(masked_code_encoding['input_ids'])\n",
    "\n",
    "#masked code\n",
    "masked_code = bert_tokenizer.tokenizer.decode(list(filter(lambda token_id: False if token_id == bert_tokenizer.tokenizer.bos_token_id or \n",
    "            token_id == bert_tokenizer.tokenizer.eos_token_id else True, masked_code_encoding['input_ids'])))\n",
    "\n",
    "print(masked_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CodeCheckList.predictor import Predictor\n",
    "\n",
    "predictor = Predictor.from_pretrained(checkpoint, bert_tokenizer)\n",
    "predictions = predictor(masked_code_encoding, bert_tokenizer.tokenizer(code, return_tensors='pt'), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- CODE -------------\n",
      "def multiply_numbers(a,b):\n",
      "    return a*b\n",
      "\n",
      "---------- MASKED -------------\n",
      "def multiply_numbers(a,b):\n",
      "    return a<mask>b\n",
      "\n",
      "--------- PREDICTED -----------\n",
      "def multiply_numbers(a,b):\n",
      "    return a,b\n",
      "\n",
      "--------- AST COMPARE -----------\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import CodeCheckList.utils as utils\n",
    "\n",
    "prediction_number = 0\n",
    "print('------------- CODE -------------')\n",
    "print(code)\n",
    "print('\\n---------- MASKED -------------')\n",
    "print(masked_code)\n",
    "print('\\n--------- PREDICTED -----------')\n",
    "predicted_code = predictions[prediction_number]\n",
    "print(predicted_code)\n",
    "print('\\n--------- AST COMPARE -----------')\n",
    "filtered_nodes = []\n",
    "filtered_nodes_predict = []\n",
    "utils.find_nodes(bert_tokenizer.parser.parse(bytes(code, \"utf8\")).root_node, bert_tokenizer.node_types[bert_tokenizer.node_types.index(target_node_type)], filtered_nodes)\n",
    "utils.find_nodes(bert_tokenizer.parser.parse(bytes(predicted_code, \"utf8\")).root_node, bert_tokenizer.node_types[bert_tokenizer.node_types.index(target_node_type)], filtered_nodes_predict)\n",
    "print(len(filtered_nodes))\n",
    "print(len(filtered_nodes_predict))\n",
    "#base the evaluation on size comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'ellipsis': []}, {'pattern': []}, {'exec': []}, {'pass': []}, {'call': []}, {'expression_statement': []}, {'assignment': []}, {'as': []}, {'dictionary_comprehension': []}, {'block': [(1, 1, True), (1, 1, True), (1, 1, True), (1, 1, True), (1, 1, True)]}, {'expression': []}, {'format_expression': []}, {'while_statement': []}, {'/': []}, {'argument_list': []}, {':': [(1, 1, True), (1, 1, True), (1, 1, True), (1, 1, True), (1, 1, True)]}, {'case_pattern': []}, {'from': []}, {'&': []}, {'!=': []}, {'print_statement': []}, {'type_conversion': []}, {'_simple_statement': []}, {'async': []}, {'-': []}, {'keyword_argument': []}, {'keyword_separator': []}, {'positional_separator': []}, {'break_statement': []}, {'match_statement': []}, {'unary_operator': []}, {'except': []}, {'<<': []}, {'**=': []}, {'<': []}, {'import': []}, {'exec_statement': []}, {'^=': []}, {'@': []}, {'except_clause': []}, {'dictionary_splat_pattern': []}, {'continue': []}, {'concatenated_string': []}, {'is': []}, {'parameter': []}, {'import_prefix': []}, {'list_splat_pattern': []}, {'pair': []}, {'class': []}, {'yield': []}, {'return': [(1, 1, True), (1, 0, False), (1, 0, False), (1, 0, False), (1, 0, False)]}, {'-=': []}, {'format_specifier': []}, {'=': []}, {'set': []}, {'binary_operator': [(1, 0, False), (1, 0, False), (1, 1, True), (1, 0, False), (1, 0, False)]}, {'list_splat': []}, {'else': []}, {'_compound_statement': []}, {'case': []}, {'typed_default_parameter': []}, {'delete_statement': []}, {'lambda_parameters': []}, {'>>=': []}, {'print': []}, {'relative_import': []}, {'subscript': []}, {'primary_expression': []}, {'{': []}, {'boolean_operator': []}, {'case_clause': []}, {'string': []}, {'escape_sequence': []}, {'import_from_statement': []}, {'expression_list': []}, {',': [(1, 1, True), (1, 0, False), (1, 0, False), (1, 1, True), (1, 1, True)]}, {'->': []}, {'break': []}, {'elif_clause': []}, {'comment': []}, {'dictionary': []}, {'aliased_import': []}, {'interpolation': []}, {'finally': []}, {'lambda': []}, {'pass_statement': []}, {'{{': []}, {'chevron': []}, {'continue_statement': []}, {'raise_statement': []}, {'parenthesized_expression': []}, {'augmented_assignment': []}, {'<=': []}, {'list': []}, {'try_statement': []}, {'integer': []}, {'typed_parameter': []}, {'else_clause': []}, {'attribute': []}, {'|': []}, {'finally_clause': []}, {'elif': []}, {'==': []}, {'not_operator': []}, {'and': []}, {'future_import_statement': []}, {'if': []}, {'in': []}, {'pattern_list': []}, {'assert_statement': []}, {'/=': []}, {'module': [(1, 1, True), (1, 1, True), (1, 1, True), (1, 1, True), (1, 1, True)]}, {'list_comprehension': []}, {'&=': []}, {'__future__': []}, {'list_pattern': []}, {'return_statement': [(1, 1, True), (1, 1, True), (1, 1, True), (1, 0, False), (1, 0, False)]}, {'[': []}, {'//=': []}, {')': [(1, 1, True), (1, 1, True), (1, 1, True), (1, 1, True), (1, 1, True)]}, {'import_statement': []}, {':=': []}, {'or': []}, {'}': []}, {'//': []}, {'with_statement': []}, {'>>': []}, {'<<=': []}, {'generator_expression': []}, {'+=': []}, {'nonlocal_statement': []}, {'await': []}, {'decorator': []}, {'**': []}, {'raise': []}, {'dictionary_splat': []}, {'named_expression': []}, {'parameters': [(1, 1, True), (1, 1, True), (1, 0, False), (1, 1, True), (1, 0, False)]}, {']': []}, {'@=': []}, {'decorated_definition': []}, {'for': []}, {'%=': []}, {'+': []}, {'conditional_expression': []}, {';': []}, {'*': [(1, 0, False), (1, 1, True), (1, 0, False), (1, 0, False), (1, 0, False)]}, {'tuple_pattern': []}, {'default_parameter': []}, {'|=': []}, {'class_definition': []}, {'identifier': [(5, 5, True), (5, 5, True), (5, 6, True), (5, 5, True), (5, 6, True)]}, {'for_in_clause': []}, {'dotted_name': []}, {'with_item': []}, {'.': []}, {'*=': []}, {'\"': []}, {'false': []}, {'>=': []}, {'try': []}, {'if_statement': []}, {'if_clause': []}, {'slice': []}, {'del': []}, {'def': [(1, 1, True), (1, 0, False), (1, 1, True), (1, 0, False), (1, 0, False)]}, {'function_definition': [(1, 0, False), (1, 0, False), (1, 0, False), (1, 0, False), (1, 0, False)]}, {'comparison_operator': []}, {'tuple': []}, {'parenthesized_list_splat': []}, {'global': []}, {'with': []}, {'<>': []}, {'global_statement': []}, {'~': []}, {'set_comprehension': []}, {'(': [(1, 1, True), (1, 1, True), (1, 1, True), (1, 1, True), (1, 1, True)]}, {'type': []}, {'wildcard_import': []}, {'not': []}, {'%': []}, {'as_pattern': []}, {'match': []}, {'while': []}, {'}}': []}, {'with_clause': []}, {'assert': []}, {'nonlocal': []}, {'^': []}, {'true': []}, {'float': []}, {'none': []}, {'for_statement': []}, {'>': []}]\n"
     ]
    }
   ],
   "source": [
    "from CodeCheckList.evaluator import Evaluator\n",
    "\n",
    "evaluator = Evaluator(checkpoint, python_language)\n",
    "\n",
    "results = evaluator(code, 5)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import CodeCheckList\n",
    "\n",
    "import CodeCheckList.utils as utils\n",
    "from CodeCheckList.tokenizer import CodeTokenizer\n",
    "from CodeCheckList.masker import Masker\n",
    "from CodeCheckList.predictor import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Evaluator:\n",
    "    \"\"\"Evaluator Module to perform all AST Evaluations\"\"\"\n",
    "    def __init__(self, checkpoint: str, language):\n",
    "        self.tokenizer = CodeTokenizer.from_pretrained(checkpoint, language)\n",
    "        self.masker = Masker(self.tokenizer)\n",
    "        self.predictor = Predictor.from_pretrained(checkpoint, self.tokenizer)\n",
    "\n",
    "    def __call__(self, source_code: str, number_of_predictions: int):\n",
    "        evaluation_results = []\n",
    "        for node_type_idx, node_type in enumerate(self.tokenizer.node_types):\n",
    "            evaluation_results.append({node_type: self.evaluate_snippet(source_code, node_type_idx, number_of_predictions)})\n",
    "        return evaluation_results\n",
    "            \n",
    "    def evaluate_snippet(self, source_code: str, target_node_type_idx: int, number_of_predictions: int):\n",
    "        results=[]\n",
    "        source_code_nodes = []\n",
    "        utils.find_nodes(self.tokenizer.parser.parse(bytes(source_code, \"utf8\")).root_node, \n",
    "            self.tokenizer.node_types[target_node_type_idx], source_code_nodes)\n",
    "        if len(source_code_nodes) == 0:\n",
    "            return results\n",
    "\n",
    "        #source_code_encoding = self.tokenizer(source_code)\n",
    "        masked_code_encoding = self.masker(source_code, self.tokenizer(source_code), target_node_type_idx)\n",
    "        #masked_code = self.tokenizer.tokenizer.decode(list(filter(lambda token_id: False if token_id == self.tokenizer.tokenizer.bos_token_id or \n",
    "        #    token_id == self.tokenizer.tokenizer.eos_token_id else True, masked_code_encoding['input_ids'])))\n",
    "        predictions = self.predictor(masked_code_encoding, self.tokenizer.tokenizer(source_code, return_tensors='pt'), number_of_predictions)       \n",
    "\n",
    "        for prediction_number in range(0, number_of_predictions):\n",
    "            predicted_code = predictions[prediction_number]\n",
    "            predicted_nodes = []\n",
    "            utils.find_nodes(self.tokenizer.parser.parse(bytes(predicted_code, \"utf8\")).root_node, self.tokenizer.node_types[target_node_type_idx], predicted_nodes)\n",
    "            results.append((len(source_code_nodes), len(predicted_nodes), len(predicted_nodes)>=len(source_code_nodes)))\n",
    "\n",
    "        return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5dfa21b9ea908da29bde2e75ccf59a8bff4851a5152f1f941db0158f4a372e7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
