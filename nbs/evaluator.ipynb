{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/svelascodimate/miniconda3/envs/code-check-list/lib/python3.10/site-packages/CodeCheckList/grammars\n"
     ]
    }
   ],
   "source": [
    "from CodeCheckList import loader\n",
    "\n",
    "\"\"\"define language\"\"\"\n",
    "python_language = \"python\"\n",
    "\n",
    "languages = [python_language]\n",
    "\n",
    "loader.download_grammars(languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"define the model checkpoint\"\"\"\n",
    "checkpoint = \"huggingface/CodeBERTa-small-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CodeCheckList.tokenizer import CodeTokenizer\n",
    "from CodeCheckList.masker import Masker\n",
    "\n",
    "#create code tokenizer \n",
    "bert_tokenizer = CodeTokenizer.from_pretrained(checkpoint, python_language)\n",
    "\n",
    "#create code masker\n",
    "code_masker = Masker(bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['**', 'assert', 'for', 'global', 'integer', 'except', 'future_import_statement', 'parameters', '>=', 'for_in_clause', 'try_statement', '*', 'from', 'list_splat_pattern', '.', 'dictionary_splat_pattern', 'pass_statement', 'as', 'global_statement', 'conditional_expression', 'set_comprehension', 'import', 'if_clause', 'decorated_definition', 'else_clause', 'subscript', '+', 'break_statement', 'class_definition', 'attribute', 'expression_statement', 'decorator', 'type', 'primary_expression', 'if_statement', '//=', '<<', 'parameter', 'match_statement', '&', '/', '\"', 'format_specifier', 'pattern_list', 'raise_statement', '!=', '_compound_statement', '|', 'wildcard_import', 'typed_parameter', 'none', 'tuple', '|=', 'if', 'with', 'nonlocal', 'format_expression', 'dotted_name', 'continue', 'float', '__future__', '->', 'nonlocal_statement', 'false', '**=', '>>', 'elif', '>', 'identifier', '_simple_statement', 'import_from_statement', 'slice', 'while_statement', 'and', 'import_prefix', 'else', '&=', 'comparison_operator', 'assignment', '>>=', 'dictionary', 'binary_operator', 'case_clause', '@', 'list_pattern', 'type_conversion', 'case', 'with_clause', 'yield', 'assert_statement', 'pass', 'parenthesized_expression', 'block', 'argument_list', 'not_operator', 'relative_import', 'async', 'comment', 'return_statement', 'exec', 'await', 'list_comprehension', 'is', 'expression_list', 'elif_clause', 'dictionary_splat', 'list', 'boolean_operator', '<>', '{', '~', 'string', 'print_statement', '*=', '}', 'keyword_separator', 'positional_separator', '%=', 'tuple_pattern', 'as_pattern', 'interpolation', '<<=', 'class', 'aliased_import', 'print', 'finally', 'in', '<=', 'expression', '=', 'concatenated_string', '@=', 'function_definition', 'return', 'ellipsis', 'lambda', '^', 'dictionary_comprehension', 'case_pattern', 'typed_default_parameter', ',', 'with_statement', '^=', 'or', 'keyword_argument', 'true', 'call', 'chevron', 'default_parameter', '[', 'continue_statement', 'pair', 'unary_operator', '-=', '==', ')', 'try', '{{', 'for_statement', 'named_expression', 'not', 'finally_clause', '/=', ';', 'escape_sequence', 'raise', 'set', ':', 'delete_statement', 'exec_statement', 'augmented_assignment', 'del', '<', '%', 'break', 'list_splat', 'module', 'pattern', 'match', '}}', ']', 'except_clause', 'generator_expression', '(', 'lambda_parameters', '+=', 'with_item', ':=', 'while', '//', 'import_statement', 'parenthesized_list_splat', '-', 'def']\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.node_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def multiply_numbers(a,b):\n",
      "    return a<mask>b\n"
     ]
    }
   ],
   "source": [
    "\"\"\"example source code\"\"\"\n",
    "\n",
    "code = \"def multiply_numbers(a,b):\\n    return a*b\"\n",
    "#code = \"def hello_world(a,b):\\n    print('hello world')\"\n",
    "#code = \"def __ordered_values_by_indexes(self, data, inds): \\\"\\\"\\\" Return values (intensities) by indexes. Used for multiscale graph cut. data = [[0 1 1], [0 2 2], [0 2 2]] inds = [[0 1 2], [3 4 4], [5 4 4]] return: [0, 1, 1, 0, 2, 0] If the data are not consistent, it will take the maximal value \\\"\\\"\\\" # get unique labels and their first indexes # lab, linds = np.unique(inds, return_index=True) # compute values by indexes # values = data.reshape(-1)[linds] # alternative slow implementation # if there are different data on same index, it will take # maximal value # lab = np.unique(inds) # values = [0]*len(lab) # for label in lab: # values[label] = np.max(data[inds == label]) # # values = np.asarray(values) # yet another implementation values = [None] * (np.max(inds) + 1) linear_inds = inds.ravel() linear_data = data.ravel() for i in range(0, len(linear_inds)): # going over all data pixels if values[linear_inds[i]] is None: # this index is found for first values[linear_inds[i]] = linear_data[i] elif values[linear_inds[i]] < linear_data[i]: # here can be changed maximal or minimal value values[linear_inds[i]] = linear_data[i] values = np.asarray(values) return values\"\n",
    "#code = \"def __ordered_values_by_indexes(self, data, inds):  # get unique labels and their first indexes # lab, linds = np.unique(inds, return_index=True) # compute values by indexes # values = data.reshape(-1)[linds] # alternative slow implementation # if there are different data on same index, it will take # maximal value # lab = np.unique(inds) # values = [0]*len(lab) # for label in lab: # values[label] = np.max(data[inds == label]) # # values = np.asarray(values) # yet another implementation values = [None] * (np.max(inds) + 1) linear_inds = inds.ravel() linear_data = data.ravel() for i in range(0, len(linear_inds)): # going over all data pixels if values[linear_inds[i]] is None: # this index is found for first values[linear_inds[i]] = linear_data[i] elif values[linear_inds[i]] < linear_data[i]: # here can be changed maximal or minimal value values[linear_inds[i]] = linear_data[i] values = np.asarray(values) return values\"\n",
    "target_node_type = \"*\"\n",
    "\n",
    "#encoding \n",
    "source_code_encoding = bert_tokenizer(code)\n",
    "\n",
    "#masking\n",
    "masked_code_encoding = code_masker(code, bert_tokenizer(code), bert_tokenizer.node_types.index(target_node_type))\n",
    "\n",
    "assert len(source_code_encoding['input_ids']) == len(masked_code_encoding['input_ids'])\n",
    "\n",
    "#masked code\n",
    "masked_code = bert_tokenizer.tokenizer.decode(list(filter(lambda token_id: False if token_id == bert_tokenizer.tokenizer.bos_token_id or \n",
    "            token_id == bert_tokenizer.tokenizer.eos_token_id else True, masked_code_encoding['input_ids'])))\n",
    "\n",
    "print(masked_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CodeCheckList.predictor import Predictor\n",
    "\n",
    "predictor = Predictor.from_pretrained(checkpoint, bert_tokenizer)\n",
    "predictions = predictor(masked_code_encoding, bert_tokenizer.tokenizer(code, return_tensors='pt'), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- CODE -------------\n",
      "def multiply_numbers(a,b):\n",
      "    return a*b\n",
      "\n",
      "---------- MASKED -------------\n",
      "def multiply_numbers(a,b):\n",
      "    return a<mask>b\n",
      "\n",
      "--------- PREDICTED -----------\n",
      "def multiply_numbers(a,b):\n",
      "    return a,b\n",
      "\n",
      "--------- AST COMPARE -----------\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import CodeCheckList.utils as utils\n",
    "\n",
    "prediction_number = 0\n",
    "print('------------- CODE -------------')\n",
    "print(code)\n",
    "print('\\n---------- MASKED -------------')\n",
    "print(masked_code)\n",
    "print('\\n--------- PREDICTED -----------')\n",
    "predicted_code = predictions[prediction_number]\n",
    "print(predicted_code)\n",
    "print('\\n--------- AST COMPARE -----------')\n",
    "filtered_nodes = []\n",
    "filtered_nodes_predict = []\n",
    "utils.find_nodes(bert_tokenizer.parser.parse(bytes(code, \"utf8\")).root_node, bert_tokenizer.node_types[bert_tokenizer.node_types.index(target_node_type)], filtered_nodes)\n",
    "utils.find_nodes(bert_tokenizer.parser.parse(bytes(predicted_code, \"utf8\")).root_node, bert_tokenizer.node_types[bert_tokenizer.node_types.index(target_node_type)], filtered_nodes_predict)\n",
    "print(len(filtered_nodes))\n",
    "print(len(filtered_nodes_predict))\n",
    "#base the evaluation on size comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'**': []}, {'assert': []}, {'for': []}, {'global': []}, {'integer': []}, {'except': []}, {'future_import_statement': []}, {'parameters': [(1, 1, True), (1, 1, True), (1, 0, False), (1, 1, True), (1, 0, False)]}, {'>=': []}, {'for_in_clause': []}, {'try_statement': []}, {'*': [(1, 0, False), (1, 1, True), (1, 0, False), (1, 0, False), (1, 0, False)]}, {'from': []}, {'list_splat_pattern': []}, {'.': []}, {'dictionary_splat_pattern': []}, {'pass_statement': []}, {'as': []}, {'global_statement': []}, {'conditional_expression': []}, {'set_comprehension': []}, {'import': []}, {'if_clause': []}, {'decorated_definition': []}, {'else_clause': []}, {'subscript': []}, {'+': []}, {'break_statement': []}, {'class_definition': []}, {'attribute': []}, {'expression_statement': []}, {'decorator': []}, {'type': []}, {'primary_expression': []}, {'if_statement': []}, {'//=': []}, {'<<': []}, {'parameter': []}, {'match_statement': []}, {'&': []}, {'/': []}, {'\"': []}, {'format_specifier': []}, {'pattern_list': []}, {'raise_statement': []}, {'!=': []}, {'_compound_statement': []}, {'|': []}, {'wildcard_import': []}, {'typed_parameter': []}, {'none': []}, {'tuple': []}, {'|=': []}, {'if': []}, {'with': []}, {'nonlocal': []}, {'format_expression': []}, {'dotted_name': []}, {'continue': []}, {'float': []}, {'__future__': []}, {'->': []}, {'nonlocal_statement': []}, {'false': []}, {'**=': []}, {'>>': []}, {'elif': []}, {'>': []}, {'identifier': [(5, 5, True), (5, 5, True), (5, 6, True), (5, 5, True), (5, 6, True)]}, {'_simple_statement': []}, {'import_from_statement': []}, {'slice': []}, {'while_statement': []}, {'and': []}, {'import_prefix': []}, {'else': []}, {'&=': []}, {'comparison_operator': []}, {'assignment': []}, {'>>=': []}, {'dictionary': []}, {'binary_operator': [(1, 0, False), (1, 0, False), (1, 1, True), (1, 0, False), (1, 0, False)]}, {'case_clause': []}, {'@': []}, {'list_pattern': []}, {'type_conversion': []}, {'case': []}, {'with_clause': []}, {'yield': []}, {'assert_statement': []}, {'pass': []}, {'parenthesized_expression': []}, {'block': [(1, 1, True), (1, 1, True), (1, 1, True), (1, 1, True), (1, 1, True)]}, {'argument_list': []}, {'not_operator': []}, {'relative_import': []}, {'async': []}, {'comment': []}, {'return_statement': [(1, 1, True), (1, 1, True), (1, 1, True), (1, 0, False), (1, 0, False)]}, {'exec': []}, {'await': []}, {'list_comprehension': []}, {'is': []}, {'expression_list': []}, {'elif_clause': []}, {'dictionary_splat': []}, {'list': []}, {'boolean_operator': []}, {'<>': []}, {'{': []}, {'~': []}, {'string': []}, {'print_statement': []}, {'*=': []}, {'}': []}, {'keyword_separator': []}, {'positional_separator': []}, {'%=': []}, {'tuple_pattern': []}, {'as_pattern': []}, {'interpolation': []}, {'<<=': []}, {'class': []}, {'aliased_import': []}, {'print': []}, {'finally': []}, {'in': []}, {'<=': []}, {'expression': []}, {'=': []}, {'concatenated_string': []}, {'@=': []}, {'function_definition': [(1, 0, False), (1, 0, False), (1, 0, False), (1, 0, False), (1, 0, False)]}, {'return': [(1, 1, True), (1, 0, False), (1, 0, False), (1, 0, False), (1, 0, False)]}, {'ellipsis': []}, {'lambda': []}, {'^': []}, {'dictionary_comprehension': []}, {'case_pattern': []}, {'typed_default_parameter': []}, {',': [(1, 1, True), (1, 0, False), (1, 0, False), (1, 1, True), (1, 1, True)]}, {'with_statement': []}, {'^=': []}, {'or': []}, {'keyword_argument': []}, {'true': []}, {'call': []}, {'chevron': []}, {'default_parameter': []}, {'[': []}, {'continue_statement': []}, {'pair': []}, {'unary_operator': []}, {'-=': []}, {'==': []}, {')': [(1, 1, True), (1, 1, True), (1, 1, True), (1, 1, True), (1, 1, True)]}, {'try': []}, {'{{': []}, {'for_statement': []}, {'named_expression': []}, {'not': []}, {'finally_clause': []}, {'/=': []}, {';': []}, {'escape_sequence': []}, {'raise': []}, {'set': []}, {':': [(1, 1, True), (1, 1, True), (1, 1, True), (1, 1, True), (1, 1, True)]}, {'delete_statement': []}, {'exec_statement': []}, {'augmented_assignment': []}, {'del': []}, {'<': []}, {'%': []}, {'break': []}, {'list_splat': []}, {'module': [(1, 1, True), (1, 1, True), (1, 1, True), (1, 1, True), (1, 1, True)]}, {'pattern': []}, {'match': []}, {'}}': []}, {']': []}, {'except_clause': []}, {'generator_expression': []}, {'(': [(1, 1, True), (1, 1, True), (1, 1, True), (1, 1, True), (1, 1, True)]}, {'lambda_parameters': []}, {'+=': []}, {'with_item': []}, {':=': []}, {'while': []}, {'//': []}, {'import_statement': []}, {'parenthesized_list_splat': []}, {'-': []}, {'def': [(1, 1, True), (1, 0, False), (1, 1, True), (1, 0, False), (1, 0, False)]}]\n"
     ]
    }
   ],
   "source": [
    "from CodeCheckList.evaluator import Evaluator\n",
    "\n",
    "evaluator = Evaluator(checkpoint, python_language)\n",
    "\n",
    "results = evaluator(code, 5)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import CodeCheckList\n",
    "\n",
    "import CodeCheckList.utils as utils\n",
    "from CodeCheckList.tokenizer import CodeTokenizer\n",
    "from CodeCheckList.masker import Masker\n",
    "from CodeCheckList.predictor import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Evaluator:\n",
    "    \"\"\"Evaluator Module to perform all AST Evaluations\"\"\"\n",
    "    def __init__(self, checkpoint: str, language):\n",
    "        self.tokenizer = CodeTokenizer.from_pretrained(checkpoint, language)\n",
    "        self.masker = Masker(self.tokenizer)\n",
    "        self.predictor = Predictor.from_pretrained(checkpoint, self.tokenizer)\n",
    "\n",
    "    def __call__(self, source_code: str, number_of_predictions: int):\n",
    "        evaluation_results = []\n",
    "        for node_type_idx, node_type in enumerate(self.tokenizer.node_types):\n",
    "            evaluation_results.append({node_type: self.evaluate_snippet(source_code, node_type_idx, number_of_predictions)})\n",
    "        return evaluation_results\n",
    "            \n",
    "    def evaluate_snippet(self, source_code: str, target_node_type_idx: int, number_of_predictions: int):\n",
    "        results=[]\n",
    "        source_code_nodes = []\n",
    "        utils.find_nodes(self.tokenizer.parser.parse(bytes(source_code, \"utf8\")).root_node, \n",
    "            self.tokenizer.node_types[target_node_type_idx], source_code_nodes)\n",
    "        if len(source_code_nodes) == 0:\n",
    "            return results\n",
    "\n",
    "        #source_code_encoding = self.tokenizer(source_code)\n",
    "        masked_code_encoding = self.masker(source_code, self.tokenizer(source_code), target_node_type_idx)\n",
    "        #masked_code = self.tokenizer.tokenizer.decode(list(filter(lambda token_id: False if token_id == self.tokenizer.tokenizer.bos_token_id or \n",
    "        #    token_id == self.tokenizer.tokenizer.eos_token_id else True, masked_code_encoding['input_ids'])))\n",
    "        predictions = self.predictor(masked_code_encoding, self.tokenizer.tokenizer(source_code, return_tensors='pt'), number_of_predictions)       \n",
    "\n",
    "        for prediction_number in range(0, number_of_predictions):\n",
    "            predicted_code = predictions[prediction_number]\n",
    "            predicted_nodes = []\n",
    "            utils.find_nodes(self.tokenizer.parser.parse(bytes(predicted_code, \"utf8\")).root_node, self.tokenizer.node_types[target_node_type_idx], predicted_nodes)\n",
    "            results.append((len(source_code_nodes), len(predicted_nodes), len(predicted_nodes)>=len(source_code_nodes)))\n",
    "\n",
    "        return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5dfa21b9ea908da29bde2e75ccf59a8bff4851a5152f1f941db0158f4a372e7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
