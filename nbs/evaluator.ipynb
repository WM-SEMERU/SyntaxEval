{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluator Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.3) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import CodeCheckList.utils as utils\n",
    "from CodeCheckList.tokenizer import CodeTokenizer\n",
    "from CodeCheckList.masker import Masker\n",
    "from CodeCheckList.predictor import Predictor\n",
    "from CodeCheckList.judge import Judge\n",
    "\n",
    "import statistics\n",
    "import textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Evaluator:\n",
    "    \"\"\"Evaluator Module to perform all AST Evaluations\"\"\"\n",
    "    def __init__(self, checkpoint: str, language, gpu_available=False):\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "        self.tokenizer = CodeTokenizer.from_pretrained(checkpoint, language)\n",
    "        self.masker = Masker(self.tokenizer)\n",
    "        self.predictor = Predictor.from_pretrained(checkpoint, self.tokenizer, gpu_available)\n",
    "        self.judge = Judge(self.tokenizer)\n",
    "\n",
    "    def __call__(self, test_set, concepts: list, masking_rate: float, code_field: str):\n",
    "        result_list = self.evaluate_concepts_in_test_set(concepts, test_set, masking_rate, code_field)\n",
    "        results_dataframe = pd.DataFrame([], columns=[\n",
    "            'sample_id', 'ast_element', 'sample', 'masking_rate', \n",
    "            'ast_element_ocurrences','mask_jaccard', 'mask_sorensen_dice', 'mask_levenshtein', \n",
    "            'mask_random_jaccard', 'mask_random_sorensen_dice', 'mask_random_levenshtein'\n",
    "            ]) ## TODO ADD CONFOUNDERS\n",
    "        for result_index, result in enumerate(result_list):\n",
    "            results_dataframe.loc[len(results_dataframe.index)] = result\n",
    "        return results_dataframe\n",
    "    \n",
    "    def pipeline(self, test_set, number_of_predictions: int, masking_rate: float):\n",
    "        \"\"\"Deprecated\"\"\"\n",
    "        results_dict = self.evaluate_test_set(test_set, number_of_predictions, masking_rate)\n",
    "        results_dataframe = pd.DataFrame([], columns=[\n",
    "            'ast_element', 'occurences', 'jaccard', 'sorensen_dice', 'levenshtein', 'jaccard_avg', 'sorensen_dice_avg', 'levenshtein_avg'])\n",
    "        for result_index, result in enumerate(results_dict):\n",
    "            results_dataframe.loc[len(results_dataframe.index)] = [self.tokenizer.node_types[result_index], result[0], tuple(tuple(l) for l in result[1]), tuple(tuple(l) for l in result[2]), tuple(tuple(l) for l in result[3]), tuple(result[4]), tuple(result[5]), tuple(result[6])]\n",
    "        return results_dataframe\n",
    "    \n",
    "    def evaluate_test_set(self, test_set, number_of_predictions: int, masking_rate: float):\n",
    "        \"\"\"Deprecated\"\"\"\n",
    "        results_dict = []\n",
    "        for node_type in self.tokenizer.node_types:\n",
    "            results_dict.append([0,                                           #ocurrences\n",
    "                                [[] for i in range(0,number_of_predictions)], #jaccard per prediction\n",
    "                                [[] for i in range(0,number_of_predictions)], #sorensen_dice per prediction\n",
    "                                [[] for i in range(0,number_of_predictions)], #levenshtein per prediction\n",
    "                                [0 for i in range(0,number_of_predictions)],  #avg jaccard per prediction\n",
    "                                [0 for i in range(0,number_of_predictions)],  #avg sorensen_dice per prediction\n",
    "                                [0 for i in range(0,number_of_predictions)],  #avg levenshtein per prediction\n",
    "                                ])\n",
    "        for sample_index, sample in enumerate(test_set):\n",
    "            print('-------- evaluating sample:'+str(sample_index)+' --------')\n",
    "            for node_type_idx, node_type in enumerate(self.tokenizer.node_types):\n",
    "                node_type_results = self.evaluate_node_type_on_snippet(sample['whole_func_string'], node_type_idx, number_of_predictions, masking_rate)\n",
    "                if(len(node_type_results)>0):\n",
    "                    results_dict[node_type_idx][0] += node_type_results[0][0]\n",
    "                    for prediction_number_index in range(0, number_of_predictions):\n",
    "                        if(node_type_results[prediction_number_index][1]!=None):\n",
    "                            results_dict[node_type_idx][1][prediction_number_index].append(node_type_results[prediction_number_index][1])\n",
    "                            results_dict[node_type_idx][4][prediction_number_index] = round(statistics.mean(results_dict[node_type_idx][1][prediction_number_index]),3)\n",
    "                        if(node_type_results[prediction_number_index][2]!=None):\n",
    "                            results_dict[node_type_idx][2][prediction_number_index].append(node_type_results[prediction_number_index][2])\n",
    "                            results_dict[node_type_idx][5][prediction_number_index] = round(statistics.mean(results_dict[node_type_idx][2][prediction_number_index]),3)\n",
    "                        if(node_type_results[prediction_number_index][3]!=None):\n",
    "                            results_dict[node_type_idx][3][prediction_number_index].append(node_type_results[prediction_number_index][3])\n",
    "                            results_dict[node_type_idx][6][prediction_number_index] = round(statistics.mean(results_dict[node_type_idx][3][prediction_number_index]),3)\n",
    "        return results_dict\n",
    "        \n",
    "    def evaluate_node_type_on_snippet(self, source_code: str, target_node_type_idx: int, number_of_predictions: int, masking_rate: float):\n",
    "        results=[]\n",
    "        source_code_tree = self.tokenizer.parser.parse(bytes(source_code, \"utf8\"))\n",
    "        source_code_nodes = []\n",
    "        utils.find_nodes(source_code_tree.root_node, self.tokenizer.node_types[target_node_type_idx], source_code_nodes)\n",
    "        if len(source_code_nodes) == 0:\n",
    "            return results\n",
    "        masked_code_encoding, number_of_masked_tokens = self.masker.mask_ast_tokens(source_code, self.tokenizer(source_code), target_node_type_idx, masking_rate)\n",
    "        predictions = self.predictor(masked_code_encoding, self.tokenizer.tokenizer(source_code, return_tensors='pt'), number_of_predictions)  \n",
    "        for prediction_number in range(0, number_of_predictions):\n",
    "            predicted_code = predictions[prediction_number]\n",
    "            prediction_results = self.judge(source_code, predicted_code)\n",
    "            results.append([len(source_code_nodes), prediction_results[0], prediction_results[1], prediction_results[2]])\n",
    "        return results, number_of_masked_tokens\n",
    "    \n",
    "    def evaluate_random_mask_on_snippet(self, source_code: str, number_of_predictions:int, number_tokens_to_mask: int):\n",
    "        results=[]\n",
    "        masked_code_encoding = self.masker.mask_random_tokens(self.tokenizer(source_code), number_tokens_to_mask)\n",
    "        predictions = self.predictor(masked_code_encoding, self.tokenizer.tokenizer(source_code, return_tensors='pt'), number_of_predictions)\n",
    "        for prediction_number in range(0, number_of_predictions):\n",
    "            predicted_code = predictions[prediction_number]\n",
    "            prediction_results = self.judge(source_code, predicted_code)\n",
    "            results.append([0, prediction_results[0], prediction_results[1], prediction_results[2]])\n",
    "        return results\n",
    "    \n",
    "    def evaluate_concepts_in_test_set(self, concepts: list, test_set, masking_rate: float, code_field: str):\n",
    "        test_set_results = []\n",
    "        for sample_index, sample in enumerate(test_set):\n",
    "            print('-------- evaluating sample:'+str(sample_index)+' --------')\n",
    "            for concept in concepts: \n",
    "                concept_mask_results, number_of_masked_tokens = self.evaluate_node_type_on_snippet(sample[code_field], self.tokenizer.node_types.index(concept), 1, masking_rate)\n",
    "                random_mask_results = self.evaluate_random_mask_on_snippet(sample[code_field], 1, number_of_masked_tokens)\n",
    "                if len(concept_mask_results)>0:\n",
    "                    test_set_results.append([sample_index, concept, sample[code_field], masking_rate,\n",
    "                                            concept_mask_results[0][0], concept_mask_results[0][1], concept_mask_results[0][2], concept_mask_results[0][3], \n",
    "                                            random_mask_results[0][1], random_mask_results[0][2], random_mask_results[0][3]]) #TODO ADD CONFOUNDERS\n",
    "        return test_set_results\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/CodeCheckList/grammars\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "from CodeCheckList import loader\n",
    "\n",
    "\"\"\"define language\"\"\"\n",
    "python_language = \"python\"\n",
    "\n",
    "languages = [python_language]\n",
    "\n",
    "loader.download_grammars(languages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "\"\"\"define the model checkpoint\"\"\"\n",
    "checkpoint = \"huggingface/CodeBERTa-small-v1\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "from CodeCheckList.tokenizer import CodeTokenizer\n",
    "from CodeCheckList.masker import Masker\n",
    "\n",
    "#create code tokenizer \n",
    "bert_tokenizer = CodeTokenizer.from_pretrained(checkpoint, python_language)\n",
    "\n",
    "#create code masker\n",
    "code_masker = Masker(bert_tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['type', '%', 'print_statement', 'global', 'set_comprehension', 'import', '>=', '%=', 'conditional_expression', '->', ']', '{{', '*=', ':', 'type_conversion', 'set', 'expression', '(', 'dictionary_splat', '-', 'else_clause', 'break', 'case', 'raise', 'match_statement', 'tuple_pattern', 'comment', 'dictionary_splat_pattern', 'nonlocal_statement', 'default_parameter', 'dotted_name', 'dictionary', 'assert', 'true', 'string', '~', 'pair', 'with', 'future_import_statement', 'class_definition', 'identifier', 'binary_operator', 'parenthesized_list_splat', '_simple_statement', ';', 'positional_separator', 'raise_statement', 'argument_list', '**=', '**', 'for_in_clause', 'as_pattern', 'finally_clause', 'function_definition', 'if_clause', 'lambda_parameters', 'from', 'false', 'elif', 'break_statement', 'keyword_separator', 'integer', 'unary_operator', '==', 'as_pattern_target', 'parameter', 'wildcard_import', '\"', 'is', 'return_statement', 'attribute', 'import_statement', 'ellipsis', 'except', '*', '/=', '<>', 'case_pattern', 'return', '&=', 'with_statement', 'keyword_argument', 'comparison_operator', 'list_splat_pattern', 'not', '|', 'with_clause', 'continue', 'not_operator', '{', 'tuple', '}}', 'assignment', 'import_from_statement', 'finally', 'concatenated_string', 'class', 'while', 'yield', 'relative_import', 'boolean_operator', '_compound_statement', 'for', 'async', 'named_expression', 'expression_statement', 'try', '<=', 'import_prefix', 'pass', '^=', 'typed_default_parameter', 'typed_parameter', 'primary_expression', 'slice', 'format_expression', 'block', 'def', 'expression_list', 'parameters', '>>', '<<=', 'global_statement', '//=', 'while_statement', 'in', '^', 'print', 'none', 'or', 'assert_statement', '__future__', 'parenthesized_expression', '<<', ',', 'exec', 'delete_statement', 'match', 'aliased_import', 'del', 'dictionary_comprehension', 'chevron', '@', 'format_specifier', 'lambda', '=', 'pattern_list', '}', '//', 'generator_expression', 'nonlocal', 'list', '&', ')', 'if', '.', '-=', 'float', 'list_pattern', '>>=', 'list_splat', '[', '>', 'augmented_assignment', 'call', '+=', '!=', 'except_clause', 'if_statement', 'case_clause', '/', '<', 'with_item', 'module', '+', 'subscript', 'and', 'escape_sequence', 'await', 'decorator', 'else', 'elif_clause', 'ERROR', 'pattern', 'continue_statement', 'for_statement', '|=', 'exec_statement', 'as', 'try_statement', ':=', '@=', 'list_comprehension', 'interpolation', 'pass_statement', 'decorated_definition']\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "print(bert_tokenizer.node_types)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def<mask><mask><mask>(<mask>,<mask>):\n",
      "    return<mask>*<mask>\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "\"\"\"example source code\"\"\"\n",
    "\n",
    "code = \"def multiply_numbers(a,b):\\n    return a*b\"\n",
    "#code = \"def scale(self, center=True, scale=True):\\n        \\\"\\\"\\\"\\nthe the\\n\\n\\n                                                                                                                                                          _\\n                     ____________=_=_===========________===______________________________==_____________________\\n_______\\n____\\n\\n___\\n\\n\\n\\n\\n\\n\\n\\n\\n        return return)\"\n",
    "#code = \"def hello_world(a,b):\\n    print('hello world')\"\n",
    "#code = \"def __ordered_values_by_indexes(self, data, inds): \\\"\\\"\\\" Return values (intensities) by indexes. Used for multiscale graph cut. data = [[0 1 1], [0 2 2], [0 2 2]] inds = [[0 1 2], [3 4 4], [5 4 4]] return: [0, 1, 1, 0, 2, 0] If the data are not consistent, it will take the maximal value \\\"\\\"\\\" # get unique labels and their first indexes # lab, linds = np.unique(inds, return_index=True) # compute values by indexes # values = data.reshape(-1)[linds] # alternative slow implementation # if there are different data on same index, it will take # maximal value # lab = np.unique(inds) # values = [0]*len(lab) # for label in lab: # values[label] = np.max(data[inds == label]) # # values = np.asarray(values) # yet another implementation values = [None] * (np.max(inds) + 1) linear_inds = inds.ravel() linear_data = data.ravel() for i in range(0, len(linear_inds)): # going over all data pixels if values[linear_inds[i]] is None: # this index is found for first values[linear_inds[i]] = linear_data[i] elif values[linear_inds[i]] < linear_data[i]: # here can be changed maximal or minimal value values[linear_inds[i]] = linear_data[i] values = np.asarray(values) return values\"\n",
    "#code = \"def __ordered_values_by_indexes(self, data, inds):  # get unique labels and their first indexes # lab, linds = np.unique(inds, return_index=True) # compute values by indexes # values = data.reshape(-1)[linds] # alternative slow implementation # if there are different data on same index, it will take # maximal value # lab = np.unique(inds) # values = [0]*len(lab) # for label in lab: # values[label] = np.max(data[inds == label]) # # values = np.asarray(values) # yet another implementation values = [None] * (np.max(inds) + 1) linear_inds = inds.ravel() linear_data = data.ravel() for i in range(0, len(linear_inds)): # going over all data pixels if values[linear_inds[i]] is None: # this index is found for first values[linear_inds[i]] = linear_data[i] elif values[linear_inds[i]] < linear_data[i]: # here can be changed maximal or minimal value values[linear_inds[i]] = linear_data[i] values = np.asarray(values) return values\"\n",
    "target_node_type = \"identifier\"\n",
    "\n",
    "#encoding \n",
    "source_code_encoding = bert_tokenizer(code)\n",
    "\n",
    "#masking\n",
    "masked_code_encoding, number_of_masked_tokens = code_masker.mask_ast_tokens(code, bert_tokenizer(code), bert_tokenizer.node_types.index(target_node_type), 1)\n",
    "\n",
    "assert len(source_code_encoding['input_ids']) == len(masked_code_encoding['input_ids'])\n",
    "\n",
    "#masked code\n",
    "masked_code = bert_tokenizer.tokenizer.decode(list(filter(lambda token_id: False if token_id == bert_tokenizer.tokenizer.bos_token_id or \n",
    "            token_id == bert_tokenizer.tokenizer.eos_token_id else True, masked_code_encoding['input_ids'])))\n",
    "\n",
    "print(masked_code)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "from CodeCheckList.predictor import Predictor\n",
    "\n",
    "predictor = Predictor.from_pretrained(checkpoint, bert_tokenizer)\n",
    "predictions = predictor(masked_code_encoding, bert_tokenizer.tokenizer(code, return_tensors='pt'), 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- CODE -------------\n",
      "def multiply_numbers(a,b):\n",
      "    return a*b\n",
      "\n",
      "---------- MASKED -------------\n",
      "def<mask><mask><mask>(<mask>,<mask>):\n",
      "    return<mask>*<mask>\n",
      "\n",
      "--------- PREDICTED -----------\n",
      "def __function(name, value):\n",
      "    return f*args\n",
      "\n",
      "--------- AST COMPARE -----------\n",
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "import CodeCheckList.utils as utils\n",
    "\n",
    "prediction_number = 0\n",
    "print('------------- CODE -------------')\n",
    "print(code)\n",
    "print('\\n---------- MASKED -------------')\n",
    "print(masked_code)\n",
    "print('\\n--------- PREDICTED -----------')\n",
    "predicted_code = predictions[prediction_number]\n",
    "print(predicted_code)\n",
    "print('\\n--------- AST COMPARE -----------')\n",
    "filtered_nodes = []\n",
    "filtered_nodes_predict = []\n",
    "utils.find_nodes(bert_tokenizer.parser.parse(bytes(code, \"utf8\")).root_node, bert_tokenizer.node_types[bert_tokenizer.node_types.index(target_node_type)], filtered_nodes)\n",
    "utils.find_nodes(bert_tokenizer.parser.parse(bytes(predicted_code, \"utf8\")).root_node, bert_tokenizer.node_types[bert_tokenizer.node_types.index(target_node_type)], filtered_nodes_predict)\n",
    "print(len(filtered_nodes))\n",
    "print(len(filtered_nodes_predict))\n",
    "#base the evaluation on size comparison"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: code_search_net/all\n",
      "Found cached dataset code_search_net (/root/.cache/huggingface/datasets/code_search_net/all/1.0.0/8f2524e6b62f65af5f5d65c53715c654db7b08dc93e0b7bcce2ab2f286a75be1)\n",
      "Parameter 'function'=<function get_test_sets.<locals>.<lambda> at 0x7f52f99b24c0> of the transform datasets.arrow_dataset.Dataset.filter@2.0.1 couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35bc71e278784d6884374a30ab89a000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/100529 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19408\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "from datasets import load_dataset \n",
    "import CodeCheckList.utils as utils\n",
    "import json\n",
    "\n",
    "\n",
    "evaluator = Evaluator(checkpoint, python_language)\n",
    "\n",
    "max_token_number = bert_tokenizer.tokenizer.max_len_single_sentence\n",
    "print(max_token_number)\n",
    "\n",
    "test_set = load_dataset(\"code_search_net\", split='test')\n",
    "test_set = utils.get_test_sets(test_set, python_language, max_token_number, bert_tokenizer)\n",
    "\n",
    "print(len(test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def get_vid_from_url(url):\n",
      "        \"\"\"Extracts video ID from URL.\n",
      "        \"\"\"\n",
      "        return match1(url, r'youtu\\.be/([^?/]+)') or \\\n",
      "          match1(url, r'youtube\\.com/embed/([^/?]+)') or \\\n",
      "          match1(url, r'youtube\\.com/v/([^/?]+)') or \\\n",
      "          match1(url, r'youtube\\.com/watch/([^/?]+)') or \\\n",
      "          parse_query_param(url, 'v') or \\\n",
      "          parse_query_param(parse_query_param(url, 'u'), 'v')\n"
     ]
    }
   ],
   "source": [
    "#|eval: false\n",
    "print(test_set[0]['whole_func_string'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "### LOADING GALERAS\n",
    "test_set = json.load(open('/workspaces/CodeCheckList/semeru-datasets/galeras_curated_raw/airflow/data_1.json',))\n",
    "test_set += json.load(open('/workspaces/CodeCheckList/semeru-datasets/galeras_curated_raw/AliceMind-Baba/dataset17.json',))\n",
    "\n",
    "#test_set = json.load(open('/workspaces/CodeCheckList/semeru-datasets/galeras_previews_iteration_bk/combinedDataset/dataset.json',))\n",
    "#test_set += json.load(open('/workspaces/CodeCheckList/semeru-datasets/galeras_previews_iteration_bk/combinedDataset/dataset0.json',))\n",
    "#test_set += json.load(open('/workspaces/CodeCheckList/semeru-datasets/galeras_previews_iteration_bk/combinedDataset/dataset1.json',))\n",
    "\n",
    "test_set = utils.get_test_sets_galeras(test_set, python_language, max_token_number, bert_tokenizer)\n",
    "test_set = test_set[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- evaluating sample:0 --------\n",
      "-------- evaluating sample:1 --------\n",
      "-------- evaluating sample:2 --------\n",
      "-------- evaluating sample:3 --------\n",
      "-------- evaluating sample:4 --------\n",
      "-------- evaluating sample:5 --------\n",
      "-------- evaluating sample:6 --------\n",
      "-------- evaluating sample:7 --------\n",
      "-------- evaluating sample:8 --------\n",
      "-------- evaluating sample:9 --------\n",
      "-------- evaluating sample:10 --------\n",
      "-------- evaluating sample:11 --------\n",
      "-------- evaluating sample:12 --------\n",
      "-------- evaluating sample:13 --------\n",
      "-------- evaluating sample:14 --------\n",
      "-------- evaluating sample:15 --------\n",
      "-------- evaluating sample:16 --------\n",
      "-------- evaluating sample:17 --------\n",
      "-------- evaluating sample:18 --------\n",
      "-------- evaluating sample:19 --------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>ast_element</th>\n",
       "      <th>sample</th>\n",
       "      <th>masking_rate</th>\n",
       "      <th>ast_element_ocurrences</th>\n",
       "      <th>mask_jaccard</th>\n",
       "      <th>mask_sorensen_dice</th>\n",
       "      <th>mask_levenshtein</th>\n",
       "      <th>mask_random_jaccard</th>\n",
       "      <th>mask_random_sorensen_dice</th>\n",
       "      <th>mask_random_levenshtein</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>identifier</td>\n",
       "      <td>def test_should_generate_secret_with_specified...</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.834951</td>\n",
       "      <td>0.950980</td>\n",
       "      <td>0.974874</td>\n",
       "      <td>0.921569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>identifier</td>\n",
       "      <td>def test_should_correctly_handle_password_with...</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0.947917</td>\n",
       "      <td>0.973262</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.860000</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>0.884211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>identifier</td>\n",
       "      <td>def assert_tasks_on_executor(self, executor, t...</td>\n",
       "      <td>1</td>\n",
       "      <td>56</td>\n",
       "      <td>0.867314</td>\n",
       "      <td>0.928943</td>\n",
       "      <td>0.864078</td>\n",
       "      <td>0.953069</td>\n",
       "      <td>0.975970</td>\n",
       "      <td>0.967033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>identifier</td>\n",
       "      <td>def test_tls(self):\\n        # These use test ...</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.918567</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.992908</td>\n",
       "      <td>0.992908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>identifier</td>\n",
       "      <td>def test_dask_executor_functions(self):\\n     ...</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.568182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>identifier</td>\n",
       "      <td>def verify_provider_classes():\\n    \\n    with...</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>0.753968</td>\n",
       "      <td>0.859729</td>\n",
       "      <td>0.742972</td>\n",
       "      <td>0.760563</td>\n",
       "      <td>0.864000</td>\n",
       "      <td>0.699482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>identifier</td>\n",
       "      <td>def test_python_callable_keyword_arguments_are...</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>0.787500</td>\n",
       "      <td>0.881119</td>\n",
       "      <td>0.690000</td>\n",
       "      <td>0.786164</td>\n",
       "      <td>0.880282</td>\n",
       "      <td>0.804054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>identifier</td>\n",
       "      <td>def get_conn(self) -&gt; Any:\\n        \\n        ...</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.887324</td>\n",
       "      <td>0.768293</td>\n",
       "      <td>0.868966</td>\n",
       "      <td>0.770270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>identifier</td>\n",
       "      <td>def setUp(self):\\n        with mock.patch(\\n  ...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.697368</td>\n",
       "      <td>0.821705</td>\n",
       "      <td>0.706667</td>\n",
       "      <td>0.656716</td>\n",
       "      <td>0.792793</td>\n",
       "      <td>0.614035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>identifier</td>\n",
       "      <td>def test_get_events(self, get_conn):\\n        ...</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>0.819005</td>\n",
       "      <td>0.900498</td>\n",
       "      <td>0.648148</td>\n",
       "      <td>0.797414</td>\n",
       "      <td>0.887290</td>\n",
       "      <td>0.787037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>identifier</td>\n",
       "      <td>def test_create_event(self, mock_get_conn):\\n ...</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>0.682635</td>\n",
       "      <td>0.811388</td>\n",
       "      <td>0.660494</td>\n",
       "      <td>0.723881</td>\n",
       "      <td>0.839827</td>\n",
       "      <td>0.739496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>identifier</td>\n",
       "      <td>def write(self, message):\\n        \\n        i...</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0.970149</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>identifier</td>\n",
       "      <td>def flush(self):\\n        \\n        buf = self...</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.614035</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.647059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>identifier</td>\n",
       "      <td>def _reduce_RootLogger(logger):\\n        retur...</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0.765432</td>\n",
       "      <td>0.867133</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.547945</td>\n",
       "      <td>0.707965</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>identifier</td>\n",
       "      <td>def _reduce_Logger(logger):\\n        if loggin...</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.976744</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.939394</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.953846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>identifier</td>\n",
       "      <td>def test_disable_verify_ssl(self):\\n        co...</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0.947917</td>\n",
       "      <td>0.973262</td>\n",
       "      <td>0.947917</td>\n",
       "      <td>0.904255</td>\n",
       "      <td>0.949721</td>\n",
       "      <td>0.912088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>identifier</td>\n",
       "      <td>def main():\\n    pass\\n\\n\\n@option_verbose</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>identifier</td>\n",
       "      <td>def free_space(verbose):\\n    run_command([\"su...</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0.894472</td>\n",
       "      <td>0.944297</td>\n",
       "      <td>0.884422</td>\n",
       "      <td>0.978022</td>\n",
       "      <td>0.988889</td>\n",
       "      <td>0.978022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>identifier</td>\n",
       "      <td>def action_set_skipped(self, tis):\\n        \\n...</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.620690</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>0.607843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>identifier</td>\n",
       "      <td>def test_should_create_valid_affinity_tolerati...</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0.866279</td>\n",
       "      <td>0.928349</td>\n",
       "      <td>0.844037</td>\n",
       "      <td>0.956656</td>\n",
       "      <td>0.977848</td>\n",
       "      <td>0.965300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sample_id ast_element                                             sample  \\\n",
       "0           0  identifier  def test_should_generate_secret_with_specified...   \n",
       "1           1  identifier  def test_should_correctly_handle_password_with...   \n",
       "2           2  identifier  def assert_tasks_on_executor(self, executor, t...   \n",
       "3           3  identifier  def test_tls(self):\\n        # These use test ...   \n",
       "4           4  identifier  def test_dask_executor_functions(self):\\n     ...   \n",
       "5           5  identifier  def verify_provider_classes():\\n    \\n    with...   \n",
       "6           6  identifier  def test_python_callable_keyword_arguments_are...   \n",
       "7           7  identifier  def get_conn(self) -> Any:\\n        \\n        ...   \n",
       "8           8  identifier  def setUp(self):\\n        with mock.patch(\\n  ...   \n",
       "9           9  identifier  def test_get_events(self, get_conn):\\n        ...   \n",
       "10         10  identifier  def test_create_event(self, mock_get_conn):\\n ...   \n",
       "11         11  identifier  def write(self, message):\\n        \\n        i...   \n",
       "12         12  identifier  def flush(self):\\n        \\n        buf = self...   \n",
       "13         13  identifier  def _reduce_RootLogger(logger):\\n        retur...   \n",
       "14         14  identifier  def _reduce_Logger(logger):\\n        if loggin...   \n",
       "15         15  identifier  def test_disable_verify_ssl(self):\\n        co...   \n",
       "16         16  identifier         def main():\\n    pass\\n\\n\\n@option_verbose   \n",
       "17         17  identifier  def free_space(verbose):\\n    run_command([\"su...   \n",
       "18         18  identifier  def action_set_skipped(self, tis):\\n        \\n...   \n",
       "19         19  identifier  def test_should_create_valid_affinity_tolerati...   \n",
       "\n",
       "    masking_rate  ast_element_ocurrences  mask_jaccard  mask_sorensen_dice  \\\n",
       "0              1                       9      0.818182            0.900000   \n",
       "1              1                       6      0.947917            0.973262   \n",
       "2              1                      56      0.867314            0.928943   \n",
       "3              1                      22      0.849398            0.918567   \n",
       "4              1                      12      0.758621            0.862745   \n",
       "5              1                      48      0.753968            0.859729   \n",
       "6              1                      57      0.787500            0.881119   \n",
       "7              1                      18      0.875000            0.933333   \n",
       "8              1                      13      0.697368            0.821705   \n",
       "9              1                      50      0.819005            0.900498   \n",
       "10             1                      35      0.682635            0.811388   \n",
       "11             1                      14      0.970149            0.984848   \n",
       "12             1                      12      0.875000            0.933333   \n",
       "13             1                      18      0.765432            0.867133   \n",
       "14             1                      14      0.954545            0.976744   \n",
       "15             1                      20      0.947917            0.973262   \n",
       "16             1                       2      0.789474            0.882353   \n",
       "17             1                      16      0.894472            0.944297   \n",
       "18             1                      13      0.850000            0.918919   \n",
       "19             1                      21      0.866279            0.928349   \n",
       "\n",
       "    mask_levenshtein  mask_random_jaccard  mask_random_sorensen_dice  \\\n",
       "0           0.834951             0.950980                   0.974874   \n",
       "1           0.937500             0.860000                   0.924731   \n",
       "2           0.864078             0.953069                   0.975970   \n",
       "3           0.849398             0.985915                   0.992908   \n",
       "4           0.724138             0.725490                   0.840909   \n",
       "5           0.742972             0.760563                   0.864000   \n",
       "6           0.690000             0.786164                   0.880282   \n",
       "7           0.887324             0.768293                   0.868966   \n",
       "8           0.706667             0.656716                   0.792793   \n",
       "9           0.648148             0.797414                   0.887290   \n",
       "10          0.660494             0.723881                   0.839827   \n",
       "11          0.984848             1.000000                   1.000000   \n",
       "12          0.888889             0.614035                   0.760870   \n",
       "13          0.727273             0.547945                   0.707965   \n",
       "14          0.939394             0.939394                   0.968750   \n",
       "15          0.947917             0.904255                   0.949721   \n",
       "16          0.789474             0.650000                   0.787879   \n",
       "17          0.884422             0.978022                   0.988889   \n",
       "18          0.816667             0.620690                   0.765957   \n",
       "19          0.844037             0.956656                   0.977848   \n",
       "\n",
       "    mask_random_levenshtein  \n",
       "0                  0.921569  \n",
       "1                  0.884211  \n",
       "2                  0.967033  \n",
       "3                  0.992908  \n",
       "4                  0.568182  \n",
       "5                  0.699482  \n",
       "6                  0.804054  \n",
       "7                  0.770270  \n",
       "8                  0.614035  \n",
       "9                  0.787037  \n",
       "10                 0.739496  \n",
       "11                 1.000000  \n",
       "12                 0.647059  \n",
       "13                 0.500000  \n",
       "14                 0.953846  \n",
       "15                 0.912088  \n",
       "16                 0.611111  \n",
       "17                 0.978022  \n",
       "18                 0.607843  \n",
       "19                 0.965300  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|eval: false\n",
    "number_of_predictions = 3\n",
    "checkpoint = \"huggingface/CodeBERTa-small-v1\"\n",
    "python_language = \"python\"\n",
    "masking_rate = 1\n",
    "\n",
    "evaluator = Evaluator(checkpoint, python_language, gpu_available=False)\n",
    "\n",
    "#results_dataframe = evaluator(test_set, number_of_predictions, masking_rate)\n",
    "#results_dataframe = evaluator(test_set, ['identifier'], masking_rate, 'whole_func_string')\n",
    "results_dataframe = evaluator(test_set, ['identifier'], masking_rate, 'code')\n",
    "\n",
    "#results_dataframe.sort_values(by=['occurences'], ascending=False)\n",
    "\n",
    "results_dataframe\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5dfa21b9ea908da29bde2e75ccf59a8bff4851a5152f1f941db0158f4a372e7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
