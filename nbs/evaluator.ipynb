{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/svelascodimate/miniconda3/envs/code-check-list/lib/python3.10/site-packages/CodeCheckList/grammars\n"
     ]
    }
   ],
   "source": [
    "from CodeCheckList import loader\n",
    "\n",
    "\"\"\"define language\"\"\"\n",
    "python_language = \"python\"\n",
    "\n",
    "languages = [python_language]\n",
    "\n",
    "loader.download_grammars(languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"define the model checkpoint\"\"\"\n",
    "checkpoint = \"huggingface/CodeBERTa-small-v1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CodeCheckList.tokenizer import CodeTokenizer\n",
    "from CodeCheckList.masker import Masker\n",
    "\n",
    "#create code tokenizer \n",
    "bert_tokenizer = CodeTokenizer.from_pretrained(checkpoint, python_language)\n",
    "\n",
    "#create code masker\n",
    "code_masker = Masker(bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['*', '=', 'finally', 'decorator', '>>=', 'assert_statement', 'print_statement', 'block', '**', ')', 'tuple', 'list_comprehension', 'case_pattern', 'raise_statement', 'list_splat_pattern', 'class', '%', '|=', '_simple_statement', 'else', '_compound_statement', ']', 'binary_operator', 'future_import_statement', 'print', 'integer', ':', '__future__', 'from', 'identifier', 'dotted_name', '[', '~', 'continue', 'except_clause', 'break_statement', 'elif', 'dictionary', 'type', 'module', '>', 'parenthesized_expression', 'with', '(', 'delete_statement', 'keyword_separator', 'with_statement', 'relative_import', 'try', 'pair', 'global_statement', 'dictionary_comprehension', '<<=', '<', 'parameter', '!=', 'while_statement', 'parenthesized_list_splat', 'comparison_operator', 'break', 'exec_statement', 'elif_clause', '+', 'del', 'with_clause', '+=', 'raise', 'chevron', '^=', '@=', 'match', 'ellipsis', '*=', 'match_statement', 'await', 'assert', 'comment', '<<', 'not', '&', 'positional_separator', '>=', '%=', 'while', 'finally_clause', 'expression_list', 'except', '|', 'in', '**=', 'lambda_parameters', 'case', 'expression_statement', 'pattern_list', 'dictionary_splat', 'def', 'dictionary_splat_pattern', 'return_statement', 'assignment', 'string', 'return', 'format_specifier', 'and', 'if_statement', 'wildcard_import', 'list_pattern', 'expression', 'boolean_operator', 'import_prefix', 'not_operator', 'as_pattern_target', '==', 'nonlocal', 'as', '}}', 'named_expression', 'set', 'typed_default_parameter', ';', 'argument_list', '/=', 'for_in_clause', 'false', 'case_clause', '^', 'type_conversion', 'as_pattern', '->', 'else_clause', 'default_parameter', 'format_expression', 'parameters', '&=', 'escape_sequence', 'primary_expression', 'yield', 'conditional_expression', ',', 'list_splat', 'pass_statement', '\"', 'for', 'aliased_import', 'try_statement', '.', 'if', 'generator_expression', 'decorated_definition', '-', 'none', 'exec', 'set_comprehension', 'subscript', '@', '-=', 'true', 'nonlocal_statement', '{', '//', 'for_statement', '/', ':=', 'call', 'tuple_pattern', 'import_statement', 'concatenated_string', 'import_from_statement', 'list', '}', 'typed_parameter', 'with_item', '<=', 'interpolation', 'attribute', '>>', 'if_clause', '<>', 'global', 'or', 'float', 'pass', 'pattern', 'async', 'keyword_argument', 'slice', 'class_definition', 'import', 'function_definition', 'lambda', 'is', 'augmented_assignment', '//=', 'unary_operator', 'continue_statement', '{{']\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.node_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def multiply_numbers(a,b):\n",
      "    return a<mask>b\n"
     ]
    }
   ],
   "source": [
    "\"\"\"example source code\"\"\"\n",
    "\n",
    "code = \"def multiply_numbers(a,b):\\n    return a*b\"\n",
    "#code = \"def hello_world(a,b):\\n    print('hello world')\"\n",
    "#code = \"def __ordered_values_by_indexes(self, data, inds): \\\"\\\"\\\" Return values (intensities) by indexes. Used for multiscale graph cut. data = [[0 1 1], [0 2 2], [0 2 2]] inds = [[0 1 2], [3 4 4], [5 4 4]] return: [0, 1, 1, 0, 2, 0] If the data are not consistent, it will take the maximal value \\\"\\\"\\\" # get unique labels and their first indexes # lab, linds = np.unique(inds, return_index=True) # compute values by indexes # values = data.reshape(-1)[linds] # alternative slow implementation # if there are different data on same index, it will take # maximal value # lab = np.unique(inds) # values = [0]*len(lab) # for label in lab: # values[label] = np.max(data[inds == label]) # # values = np.asarray(values) # yet another implementation values = [None] * (np.max(inds) + 1) linear_inds = inds.ravel() linear_data = data.ravel() for i in range(0, len(linear_inds)): # going over all data pixels if values[linear_inds[i]] is None: # this index is found for first values[linear_inds[i]] = linear_data[i] elif values[linear_inds[i]] < linear_data[i]: # here can be changed maximal or minimal value values[linear_inds[i]] = linear_data[i] values = np.asarray(values) return values\"\n",
    "#code = \"def __ordered_values_by_indexes(self, data, inds):  # get unique labels and their first indexes # lab, linds = np.unique(inds, return_index=True) # compute values by indexes # values = data.reshape(-1)[linds] # alternative slow implementation # if there are different data on same index, it will take # maximal value # lab = np.unique(inds) # values = [0]*len(lab) # for label in lab: # values[label] = np.max(data[inds == label]) # # values = np.asarray(values) # yet another implementation values = [None] * (np.max(inds) + 1) linear_inds = inds.ravel() linear_data = data.ravel() for i in range(0, len(linear_inds)): # going over all data pixels if values[linear_inds[i]] is None: # this index is found for first values[linear_inds[i]] = linear_data[i] elif values[linear_inds[i]] < linear_data[i]: # here can be changed maximal or minimal value values[linear_inds[i]] = linear_data[i] values = np.asarray(values) return values\"\n",
    "target_node_type = \"*\"\n",
    "\n",
    "#encoding \n",
    "source_code_encoding = bert_tokenizer(code)\n",
    "\n",
    "#masking\n",
    "masked_code_encoding = code_masker(code, bert_tokenizer(code), bert_tokenizer.node_types.index(target_node_type))\n",
    "\n",
    "assert len(source_code_encoding['input_ids']) == len(masked_code_encoding['input_ids'])\n",
    "\n",
    "#masked code\n",
    "masked_code = bert_tokenizer.tokenizer.decode(list(filter(lambda token_id: False if token_id == bert_tokenizer.tokenizer.bos_token_id or \n",
    "            token_id == bert_tokenizer.tokenizer.eos_token_id else True, masked_code_encoding['input_ids'])))\n",
    "\n",
    "print(masked_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CodeCheckList.predictor import Predictor\n",
    "\n",
    "predictor = Predictor.from_pretrained(checkpoint, bert_tokenizer)\n",
    "predictions = predictor(masked_code_encoding, bert_tokenizer.tokenizer(code, return_tensors='pt'), 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- CODE -------------\n",
      "def multiply_numbers(a,b):\n",
      "    return a*b\n",
      "\n",
      "---------- MASKED -------------\n",
      "def multiply_numbers(a,b):\n",
      "    return a<mask>b\n",
      "\n",
      "--------- PREDICTED -----------\n",
      "def multiply_numbers(a,b):\n",
      "    return a,b\n",
      "\n",
      "--------- AST COMPARE -----------\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import CodeCheckList.utils as utils\n",
    "\n",
    "prediction_number = 0\n",
    "print('------------- CODE -------------')\n",
    "print(code)\n",
    "print('\\n---------- MASKED -------------')\n",
    "print(masked_code)\n",
    "print('\\n--------- PREDICTED -----------')\n",
    "predicted_code = predictions[prediction_number]\n",
    "print(predicted_code)\n",
    "print('\\n--------- AST COMPARE -----------')\n",
    "filtered_nodes = []\n",
    "filtered_nodes_predict = []\n",
    "utils.find_nodes(bert_tokenizer.parser.parse(bytes(code, \"utf8\")).root_node, bert_tokenizer.node_types[bert_tokenizer.node_types.index(target_node_type)], filtered_nodes)\n",
    "utils.find_nodes(bert_tokenizer.parser.parse(bytes(predicted_code, \"utf8\")).root_node, bert_tokenizer.node_types[bert_tokenizer.node_types.index(target_node_type)], filtered_nodes_predict)\n",
    "print(len(filtered_nodes))\n",
    "print(len(filtered_nodes_predict))\n",
    "#base the evaluation on size comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'*': [[1, 0, False], [1, 1, True], [1, 0, False], [1, 0, False], [1, 0, False]]}, {'=': []}, {'finally': []}, {'decorator': []}, {'>>=': []}, {'assert_statement': []}, {'print_statement': []}, {'block': [[1, 1, True], [1, 1, True], [1, 1, True], [1, 1, True], [1, 1, True]]}, {'**': []}, {')': [[1, 1, True], [1, 1, True], [1, 1, True], [1, 1, True], [1, 1, True]]}, {'tuple': []}, {'list_comprehension': []}, {'case_pattern': []}, {'raise_statement': []}, {'list_splat_pattern': []}, {'class': []}, {'%': []}, {'|=': []}, {'_simple_statement': []}, {'else': []}, {'_compound_statement': []}, {']': []}, {'binary_operator': [[1, 0, False], [1, 0, False], [1, 1, True], [1, 0, False], [1, 0, False]]}, {'future_import_statement': []}, {'print': []}, {'integer': []}, {':': [[1, 1, True], [1, 1, True], [1, 1, True], [1, 1, True], [1, 1, True]]}, {'__future__': []}, {'from': []}, {'identifier': [[5, 5, True], [5, 5, True], [5, 6, True], [5, 5, True], [5, 6, True]]}, {'dotted_name': []}, {'[': []}, {'~': []}, {'continue': []}, {'except_clause': []}, {'break_statement': []}, {'elif': []}, {'dictionary': []}, {'type': []}, {'module': [[1, 1, True], [1, 1, True], [1, 1, True], [1, 1, True], [1, 1, True]]}, {'>': []}, {'parenthesized_expression': []}, {'with': []}, {'(': [[1, 1, True], [1, 1, True], [1, 1, True], [1, 1, True], [1, 1, True]]}, {'delete_statement': []}, {'keyword_separator': []}, {'with_statement': []}, {'relative_import': []}, {'try': []}, {'pair': []}, {'global_statement': []}, {'dictionary_comprehension': []}, {'<<=': []}, {'<': []}, {'parameter': []}, {'!=': []}, {'while_statement': []}, {'parenthesized_list_splat': []}, {'comparison_operator': []}, {'break': []}, {'exec_statement': []}, {'elif_clause': []}, {'+': []}, {'del': []}, {'with_clause': []}, {'+=': []}, {'raise': []}, {'chevron': []}, {'^=': []}, {'@=': []}, {'match': []}, {'ellipsis': []}, {'*=': []}, {'match_statement': []}, {'await': []}, {'assert': []}, {'comment': []}, {'<<': []}, {'not': []}, {'&': []}, {'positional_separator': []}, {'>=': []}, {'%=': []}, {'while': []}, {'finally_clause': []}, {'expression_list': []}, {'except': []}, {'|': []}, {'in': []}, {'**=': []}, {'lambda_parameters': []}, {'case': []}, {'expression_statement': []}, {'pattern_list': []}, {'dictionary_splat': []}, {'def': [[1, 1, True], [1, 0, False], [1, 1, True], [1, 0, False], [1, 0, False]]}, {'dictionary_splat_pattern': []}, {'return_statement': [[1, 1, True], [1, 1, True], [1, 1, True], [1, 0, False], [1, 0, False]]}, {'assignment': []}, {'string': []}, {'return': [[1, 1, True], [1, 0, False], [1, 0, False], [1, 0, False], [1, 0, False]]}, {'format_specifier': []}, {'and': []}, {'if_statement': []}, {'wildcard_import': []}, {'list_pattern': []}, {'expression': []}, {'boolean_operator': []}, {'import_prefix': []}, {'not_operator': []}, {'as_pattern_target': []}, {'==': []}, {'nonlocal': []}, {'as': []}, {'}}': []}, {'named_expression': []}, {'set': []}, {'typed_default_parameter': []}, {';': []}, {'argument_list': []}, {'/=': []}, {'for_in_clause': []}, {'false': []}, {'case_clause': []}, {'^': []}, {'type_conversion': []}, {'as_pattern': []}, {'->': []}, {'else_clause': []}, {'default_parameter': []}, {'format_expression': []}, {'parameters': [[1, 1, True], [1, 1, True], [1, 0, False], [1, 1, True], [1, 0, False]]}, {'&=': []}, {'escape_sequence': []}, {'primary_expression': []}, {'yield': []}, {'conditional_expression': []}, {',': [[1, 1, True], [1, 0, False], [1, 0, False], [1, 1, True], [1, 1, True]]}, {'list_splat': []}, {'pass_statement': []}, {'\"': []}, {'for': []}, {'aliased_import': []}, {'try_statement': []}, {'.': []}, {'if': []}, {'generator_expression': []}, {'decorated_definition': []}, {'-': []}, {'none': []}, {'exec': []}, {'set_comprehension': []}, {'subscript': []}, {'@': []}, {'-=': []}, {'true': []}, {'nonlocal_statement': []}, {'{': []}, {'//': []}, {'for_statement': []}, {'/': []}, {':=': []}, {'call': []}, {'tuple_pattern': []}, {'import_statement': []}, {'concatenated_string': []}, {'import_from_statement': []}, {'list': []}, {'}': []}, {'typed_parameter': []}, {'with_item': []}, {'<=': []}, {'interpolation': []}, {'attribute': []}, {'>>': []}, {'if_clause': []}, {'<>': []}, {'global': []}, {'or': []}, {'float': []}, {'pass': []}, {'pattern': []}, {'async': []}, {'keyword_argument': []}, {'slice': []}, {'class_definition': []}, {'import': []}, {'function_definition': [[1, 0, False], [1, 0, False], [1, 0, False], [1, 0, False], [1, 0, False]]}, {'lambda': []}, {'is': []}, {'augmented_assignment': []}, {'//=': []}, {'unary_operator': []}, {'continue_statement': []}, {'{{': []}]\n"
     ]
    }
   ],
   "source": [
    "from CodeCheckList.evaluator import Evaluator\n",
    "\n",
    "evaluator = Evaluator(checkpoint, python_language)\n",
    "\n",
    "results = evaluator.evaluate_code_snippet(code, 5)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Search Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: code_search_net/all\n",
      "Found cached dataset code_search_net (/home/svelascodimate/.cache/huggingface/datasets/code_search_net/all/1.0.0/80a244ab541c6b2125350b764dc5c2b715f65f00de7a56107a28915fac173a27)\n",
      "  0%|          | 0/101 [00:00<?, ?ba/s]Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 101/101 [00:39<00:00,  2.54ba/s]\n",
      "No config specified, defaulting to: code_search_net/all\n",
      "Found cached dataset code_search_net (/home/svelascodimate/.cache/huggingface/datasets/code_search_net/all/1.0.0/80a244ab541c6b2125350b764dc5c2b715f65f00de7a56107a28915fac173a27)\n",
      "  0%|          | 0/101 [00:00<?, ?ba/s]Token indices sequence length is longer than the specified maximum sequence length for this model (517 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 101/101 [00:39<00:00,  2.53ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset \n",
    "import CodeCheckList.utils as utils\n",
    "\n",
    "max_token_number = bert_tokenizer.tokenizer.max_len_single_sentence\n",
    "print(max_token_number)\n",
    "\n",
    "test_set = load_dataset(\"code_search_net\", split='test')\n",
    "test_set = test_set.filter(lambda sample: True if sample['language']== python_language\n",
    "            and len(sample['func_code_tokens']) <= max_token_number\n",
    "            and len(bert_tokenizer.tokenizer(sample['whole_func_string'])['input_ids']) <= max_token_number else False, num_proc=1)\n",
    "\n",
    "test_set = utils.get_random_sub_set_test_set(utils.get_test_sets(load_dataset(\"code_search_net\", split='test'), \"python\", evaluator.tokenizer.tokenizer.max_len_single_sentence, evaluator.tokenizer), 2)\n",
    "\n",
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate\n",
      "              ast_element  occurences successful_predictions  \\\n",
      "0                       *           1                 [1, 1]   \n",
      "1                       =           5                 [1, 0]   \n",
      "2                 finally           0                 [0, 0]   \n",
      "3               decorator           0                 [0, 0]   \n",
      "4                     >>=           0                 [0, 0]   \n",
      "..                    ...         ...                    ...   \n",
      "190  augmented_assignment           3                 [0, 0]   \n",
      "191                   //=           0                 [0, 0]   \n",
      "192        unary_operator           1                 [1, 0]   \n",
      "193    continue_statement           0                 [0, 0]   \n",
      "194                    {{           0                 [0, 0]   \n",
      "\n",
      "    failed_predictions total_predictions success_average failure_average  \n",
      "0               [0, 0]            [1, 1]      [1.0, 1.0]      [0.0, 0.0]  \n",
      "1               [0, 1]            [1, 1]      [1.0, 0.0]      [0.0, 1.0]  \n",
      "2               [0, 0]            [0, 0]          [0, 0]          [0, 0]  \n",
      "3               [0, 0]            [0, 0]          [0, 0]          [0, 0]  \n",
      "4               [0, 0]            [0, 0]          [0, 0]          [0, 0]  \n",
      "..                 ...               ...             ...             ...  \n",
      "190             [1, 1]            [1, 1]      [0.0, 0.0]      [1.0, 1.0]  \n",
      "191             [0, 0]            [0, 0]          [0, 0]          [0, 0]  \n",
      "192             [0, 1]            [1, 1]      [1.0, 0.0]      [0.0, 1.0]  \n",
      "193             [0, 0]            [0, 0]          [0, 0]          [0, 0]  \n",
      "194             [0, 0]            [0, 0]          [0, 0]          [0, 0]  \n",
      "\n",
      "[195 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "number_of_predictions = 2\n",
    "save_path = \"example\"\n",
    "\n",
    "print(\"evaluate\")\n",
    "results_dataframe = evaluator(test_set, number_of_predictions)\n",
    "print(results_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import CodeCheckList\n",
    "import pandas as pd\n",
    "\n",
    "import CodeCheckList.utils as utils\n",
    "from CodeCheckList.tokenizer import CodeTokenizer\n",
    "from CodeCheckList.masker import Masker\n",
    "from CodeCheckList.predictor import Predictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Evaluator:\n",
    "    \"\"\"Evaluator Module to perform all AST Evaluations\"\"\"\n",
    "    def __init__(self, checkpoint: str, language):\n",
    "        self.tokenizer = CodeTokenizer.from_pretrained(checkpoint, language)\n",
    "        self.masker = Masker(self.tokenizer)\n",
    "        self.predictor = Predictor.from_pretrained(checkpoint, self.tokenizer)\n",
    "\n",
    "    def __call__(self, test_set, number_of_predictions: int):\n",
    "        results_dict = self.evaluate_test_set(test_set, number_of_predictions)\n",
    "        results_dataframe = pd.DataFrame([], columns=['ast_element', 'occurences', 'successful_predictions', 'failed_predictions', 'total_predictions', 'success_average', 'failure_average'])\n",
    "        for result_index, result in enumerate(results_dict):\n",
    "            results_dataframe.loc[len(results_dataframe.index)] = [self.tokenizer.node_types[result_index] ,result[0], result[1], result[2], result[3], result[4], result[5]]\n",
    "        return results_dataframe\n",
    "    \n",
    "    def evaluate_test_set(self, test_set, number_of_predictions: int):\n",
    "        results_dict = []\n",
    "        for node_type in self.tokenizer.node_types:\n",
    "            results_dict.append([0, [0 for i in range(0,number_of_predictions)], \n",
    "                                [0 for i in range(0,number_of_predictions)], \n",
    "                                [0 for i in range(0,number_of_predictions)], \n",
    "                                [0 for i in range(0,number_of_predictions)],\n",
    "                                [0 for i in range(0,number_of_predictions)]])\n",
    "        for sample in test_set:\n",
    "            sample_results = self.evaluate_code_snippet(sample['whole_func_string'], number_of_predictions)\n",
    "            for element_index, element_result in enumerate(sample_results):\n",
    "                element_result_values = list(element_result.values())[0]\n",
    "                if len(element_result_values) > 0:\n",
    "                    results_dict[element_index][0] += element_result_values[0][0]\n",
    "                    for prediction_number_index in range(0, number_of_predictions):\n",
    "                        results_dict[element_index][1][prediction_number_index]+= (1 if element_result_values[prediction_number_index][2] else 0)\n",
    "                        results_dict[element_index][2][prediction_number_index]+= (0 if element_result_values[prediction_number_index][2] else 1)\n",
    "                        results_dict[element_index][3][prediction_number_index]= results_dict[element_index][1][prediction_number_index] + results_dict[element_index][2][prediction_number_index]\n",
    "                        results_dict[element_index][4][prediction_number_index]= results_dict[element_index][1][prediction_number_index]/results_dict[element_index][3][prediction_number_index]\n",
    "                        results_dict[element_index][5][prediction_number_index]= results_dict[element_index][2][prediction_number_index]/results_dict[element_index][3][prediction_number_index]\n",
    "        return results_dict\n",
    "        \n",
    "    def evaluate_code_snippet(self, source_code: str, number_of_predictions: int):\n",
    "        evaluation_results = []\n",
    "        for node_type_idx, node_type in enumerate(self.tokenizer.node_types):\n",
    "            evaluation_results.append({node_type: self.evaluate_node_type_on_snippet(source_code, node_type_idx, number_of_predictions)})\n",
    "        return evaluation_results\n",
    "            \n",
    "    def evaluate_node_type_on_snippet(self, source_code: str, target_node_type_idx: int, number_of_predictions: int):\n",
    "        results=[]\n",
    "        source_code_nodes = []\n",
    "        utils.find_nodes(self.tokenizer.parser.parse(bytes(source_code, \"utf8\")).root_node, \n",
    "            self.tokenizer.node_types[target_node_type_idx], source_code_nodes)\n",
    "        if len(source_code_nodes) == 0:\n",
    "            return results\n",
    "\n",
    "        #source_code_encoding = self.tokenizer(source_code)\n",
    "        masked_code_encoding = self.masker(source_code, self.tokenizer(source_code), target_node_type_idx)\n",
    "        #masked_code = self.tokenizer.tokenizer.decode(list(filter(lambda token_id: False if token_id == self.tokenizer.tokenizer.bos_token_id or \n",
    "        #    token_id == self.tokenizer.tokenizer.eos_token_id else True, masked_code_encoding['input_ids'])))\n",
    "        predictions = self.predictor(masked_code_encoding, self.tokenizer.tokenizer(source_code, return_tensors='pt'), number_of_predictions)       \n",
    "\n",
    "        for prediction_number in range(0, number_of_predictions):\n",
    "            predicted_code = predictions[prediction_number]\n",
    "            predicted_nodes = []\n",
    "            utils.find_nodes(self.tokenizer.parser.parse(bytes(predicted_code, \"utf8\")).root_node, self.tokenizer.node_types[target_node_type_idx], predicted_nodes)\n",
    "            results.append([len(source_code_nodes), len(predicted_nodes), len(predicted_nodes)>=len(source_code_nodes)])\n",
    "\n",
    "        return results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5dfa21b9ea908da29bde2e75ccf59a8bff4851a5152f1f941db0158f4a372e7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
