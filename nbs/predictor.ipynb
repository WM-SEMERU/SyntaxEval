{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "import torch\n",
    "import CodeCheckList\n",
    "from CodeCheckList.tokenizer import CodeTokenizer\n",
    "from transformers import AutoModelForMaskedLM, BatchEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "class Predictor:\n",
    "    \"\"\"Predictor Module\"\"\"\n",
    "    def __init__(self, tokenizer, model):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, masked_code_encoding: BatchEncoding, code_encoding: BatchEncoding, top_k: int):\n",
    "        masked_indexes = list(map(lambda entry: entry[0],\n",
    "            list(filter(lambda entry: True if entry[1] == self.tokenizer.tokenizer.mask_token_id else False, enumerate(masked_code_encoding['input_ids'])))))\n",
    "        code_encoding['input_ids'][0] = torch.tensor([torch.tensor(input_id) for input_id in masked_code_encoding['input_ids']])\n",
    "        model_prediction = self.model(**code_encoding)\n",
    "\n",
    "        preditions = []\n",
    "        for k_index in range(0, top_k):\n",
    "            preditions.append(code_encoding['input_ids'][0].tolist().copy())\n",
    "\n",
    "        for masked_index in masked_indexes:\n",
    "            values, predictions = model_prediction['logits'][0][masked_index].topk(top_k)\n",
    "            for k_index in range(0, top_k):\n",
    "                preditions[k_index][masked_index] = predictions[k_index]\n",
    "\n",
    "        return list(map(lambda prediction: self.tokenizer.tokenizer.decode(list(filter(lambda token_id: False if token_id == self.tokenizer.tokenizer.bos_token_id or \n",
    "                    token_id == self.tokenizer.tokenizer.eos_token_id else True, prediction))), preditions))\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pretrained(\n",
    "        name_or_path: str,          #name or path of the model\n",
    "        tokenizer: CodeTokenizer    #tokenizer, has to be of the same type that the pretrained model\n",
    "    ): \n",
    "        \"\"\"Create a AutoModelForMaskedLM from a pretrained model.\"\"\"\n",
    "        model = AutoModelForMaskedLM.from_pretrained(name_or_path)\n",
    "        return Predictor(tokenizer, model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5dfa21b9ea908da29bde2e75ccf59a8bff4851a5152f1f941db0158f4a372e7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
