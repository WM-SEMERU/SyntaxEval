{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/svelascodimate/miniconda3/envs/code-check-list/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#| export \n",
    "import torch\n",
    "from CodeCheckList.tokenizer import CodeTokenizer\n",
    "from transformers import AutoModelForMaskedLM, BatchEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Predictor:\n",
    "    \"\"\"Predictor Module\"\"\"\n",
    "    def __init__(self, tokenizer, model, gpu_available):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.gpu_available = gpu_available\n",
    "\n",
    "    def __call__(self, masked_code_encoding: BatchEncoding, code_encoding: BatchEncoding, top_k: int):\n",
    "        masked_indexes = list(map(lambda entry: entry[0],\n",
    "            list(filter(lambda entry: True if entry[1] == self.tokenizer.tokenizer.mask_token_id else False, enumerate(masked_code_encoding['input_ids'])))))\n",
    "        code_encoding['input_ids'][0] = torch.tensor([torch.tensor(input_id) for input_id in masked_code_encoding['input_ids']])\n",
    "        tokens_tensor = code_encoding['input_ids']\n",
    "        attention_mask = code_encoding['attention_mask']\n",
    "        if(self.gpu_available):\n",
    "            tokens_tensor = tokens_tensor.to('cuda:3')\n",
    "            attention_mask = attention_mask.to('cuda:3')\n",
    "        model_input = {'input_ids' : tokens_tensor,\n",
    "          'attention_mask' : attention_mask}\n",
    "        model_prediction = self.model(**model_input)\n",
    "        preditions = []\n",
    "        for k_index in range(0, top_k):\n",
    "            preditions.append(code_encoding['input_ids'][0].tolist().copy())\n",
    "        for masked_index in masked_indexes:\n",
    "            values, predictions = model_prediction['logits'][0][masked_index].topk(top_k)\n",
    "            for k_index in range(0, top_k):\n",
    "                preditions[k_index][masked_index] = predictions[k_index]\n",
    "        prediction_list = list(map(lambda prediction: self.tokenizer.tokenizer.decode(list(filter(lambda token_id: False if token_id == self.tokenizer.tokenizer.bos_token_id or \n",
    "                    token_id == self.tokenizer.tokenizer.eos_token_id else True, prediction))), preditions))\n",
    "        return prediction_list\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pretrained(\n",
    "        name_or_path: str,          #name or path of the model\n",
    "        tokenizer: CodeTokenizer,   #tokenizer, has to be of the same type that the pretrained model\n",
    "        gpu_available = False       #indicates if gpu should be used for predictions\n",
    "    ): \n",
    "        \"\"\"Create a AutoModelForMaskedLM from a pretrained model.\"\"\"\n",
    "        model = AutoModelForMaskedLM.from_pretrained(name_or_path)\n",
    "        if(gpu_available):\n",
    "            print(\"------------------Loading Model into GPU------------------\")\n",
    "            model = AutoModelForMaskedLM.from_pretrained(name_or_path).to('cuda:3')\n",
    "        return Predictor(tokenizer, model, gpu_available)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "Thu Dec  1 15:01:11 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 510.85.02    Driver Version: 510.85.02    CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-PCI...  On   | 00000000:61:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    39W / 250W |      2MiB / 40960MiB |      0%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "print(torch.cuda.device_count())\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------Loading Model into GPU------------------\n"
     ]
    }
   ],
   "source": [
    "from CodeCheckList.masker import Masker\n",
    "\n",
    "checkpoint = \"huggingface/CodeBERTa-small-v1\"\n",
    "python_language = \"python\"\n",
    "\n",
    "bert_tokenizer = CodeTokenizer.from_pretrained(checkpoint, python_language)\n",
    "predictor = Predictor.from_pretrained(checkpoint, bert_tokenizer)\n",
    "code_masker = Masker(bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def multiply_numbers(a,b):\n",
      "    return a<mask>b\n"
     ]
    }
   ],
   "source": [
    "code = \"def multiply_numbers(a,b):\\n    return a*b\"\n",
    "target_node_type = \"*\"\n",
    "\n",
    "masked_code_encoding = code_masker(code, bert_tokenizer(code), bert_tokenizer.node_types.index(target_node_type))\n",
    "masked_code = bert_tokenizer.tokenizer.decode(list(filter(lambda token_id: False if token_id == bert_tokenizer.tokenizer.bos_token_id or \n",
    "            token_id == bert_tokenizer.tokenizer.eos_token_id else True, masked_code_encoding['input_ids'])))\n",
    "print(masked_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['def multiply_numbers(a,b):\\n    return a,b', 'def multiply_numbers(a,b):\\n    return a*b', 'def multiply_numbers(a,b):\\n    return a+b', 'def multiply_numbers(a,b):\\n    return a**b', 'def multiply_numbers(a,b):\\n    return a/b']\n"
     ]
    }
   ],
   "source": [
    "predictions = predictor(masked_code_encoding, bert_tokenizer.tokenizer(code, return_tensors='pt'), 5)\n",
    "\n",
    "print(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov  4 2022, 13:48:29) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5dfa21b9ea908da29bde2e75ccf59a8bff4851a5152f1f941db0158f4a372e7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
