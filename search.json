[
  {
    "objectID": "evaluator.html",
    "href": "evaluator.html",
    "title": "CodeCheckList",
    "section": "",
    "text": "Download Grammar\n\n\nfrom CodeCheckList import loader\n\n\"\"\"define language\"\"\"\npython_language = \"python\"\n\nlanguages = [python_language]\n\nloader.download_grammars(languages)\n\n/home/svelascodimate/miniconda3/envs/code-check-list/lib/python3.10/site-packages/CodeCheckList/grammars\n\n\n\n\nLoad Model\n\n\"\"\"define the model checkpoint\"\"\"\ncheckpoint = \"huggingface/CodeBERTa-small-v1\"\n\n\n\nCreate Modules\n\nfrom CodeCheckList.tokenizer import CodeTokenizer\nfrom CodeCheckList.masker import Masker\n\n#create code tokenizer \nbert_tokenizer = CodeTokenizer.from_pretrained(checkpoint, python_language)\n\n#create code masker\ncode_masker = Masker(bert_tokenizer)\n\n/home/svelascodimate/miniconda3/envs/code-check-list/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\n\n\nNode Types\n\nprint(bert_tokenizer.node_types)\n\n['case_clause', '}}', 'aliased_import', 'try_statement', '_compound_statement', 'primary_expression', 'in', 'exec', 'future_import_statement', 'finally', 'raise_statement', 'named_expression', '_simple_statement', '==', 'tuple_pattern', 'parameter', '>', '&', 'pair', 'pass', '<=', 'while', 'unary_operator', '=', 'async', 'lambda', '(', '+', 'or', '~', 'list_splat_pattern', 'keyword_argument', 'escape_sequence', 'integer', '<', '\"', '@', '^', '|', 'concatenated_string', 'list_pattern', 'dictionary_splat', '|=', 'argument_list', 'dictionary_splat_pattern', 'for_statement', 'global_statement', '//', ',', 'global', 'return', 'list_comprehension', 'block', '/', 'break', '>>', 'as_pattern', '*=', 'subscript', ')', '>>=', 'pattern_list', 'with_item', 'type_conversion', 'assert_statement', ':', 'if_statement', 'raise', '->', '!=', 'call', 'set_comprehension', '{', ';', 'with_clause', 'lambda_parameters', 'match', 'return_statement', 'yield', '<<=', 'except', 'def', 'import_statement', 'case', 'elif', 'positional_separator', '+=', '.', '**', 'finally_clause', 'parenthesized_expression', 'attribute', 'string', 'list_splat', 'class', 'decorator', 'none', 'and', 'nonlocal', 'augmented_assignment', 'pattern', 'if_clause', 'nonlocal_statement', 'continue_statement', '-=', '**=', '&=', 'for_in_clause', 'tuple', '<<', 'case_pattern', 'module', 'except_clause', 'class_definition', 'format_expression', '__future__', 'relative_import', 'break_statement', 'set', 'type', 'parameters', 'keyword_separator', '/=', 'elif_clause', 'list', '-', '}', 'with_statement', 'float', 'import_from_statement', 'else_clause', 'match_statement', 'expression_list', 'delete_statement', 'dictionary', 'generator_expression', 'false', 'parenthesized_list_splat', 'comparison_operator', ':=', 'for', 'dictionary_comprehension', '@=', 'ellipsis', 'assignment', 'chevron', 'if', 'binary_operator', 'typed_parameter', 'while_statement', '%', 'expression_statement', 'expression', 'interpolation', '//=', 'else', 'default_parameter', 'not', 'continue', 'print', 'import_prefix', 'slice', 'try', '[', 'pass_statement', 'typed_default_parameter', 'del', 'print_statement', 'identifier', 'import', 'boolean_operator', 'is', 'as', '*', 'not_operator', 'conditional_expression', 'comment', '^=', 'with', '%=', 'assert', '{{', 'dotted_name', '>=', 'format_specifier', 'true', ']', 'await', 'decorated_definition', '<>', 'from', 'wildcard_import', 'exec_statement', 'function_definition']\n\n\n\n\nEncodings\n\n\"\"\"example source code\"\"\"\n\ncode = \"def multiply_numbers(a,b):\\n    return a*b\"\n#code = \"def hello_world(a,b):\\n    print('hello world')\"\n#code = \"def __ordered_values_by_indexes(self, data, inds): \\\"\\\"\\\" Return values (intensities) by indexes. Used for multiscale graph cut. data = [[0 1 1], [0 2 2], [0 2 2]] inds = [[0 1 2], [3 4 4], [5 4 4]] return: [0, 1, 1, 0, 2, 0] If the data are not consistent, it will take the maximal value \\\"\\\"\\\" # get unique labels and their first indexes # lab, linds = np.unique(inds, return_index=True) # compute values by indexes # values = data.reshape(-1)[linds] # alternative slow implementation # if there are different data on same index, it will take # maximal value # lab = np.unique(inds) # values = [0]*len(lab) # for label in lab: # values[label] = np.max(data[inds == label]) # # values = np.asarray(values) # yet another implementation values = [None] * (np.max(inds) + 1) linear_inds = inds.ravel() linear_data = data.ravel() for i in range(0, len(linear_inds)): # going over all data pixels if values[linear_inds[i]] is None: # this index is found for first values[linear_inds[i]] = linear_data[i] elif values[linear_inds[i]] < linear_data[i]: # here can be changed maximal or minimal value values[linear_inds[i]] = linear_data[i] values = np.asarray(values) return values\"\n#code = \"def __ordered_values_by_indexes(self, data, inds):  # get unique labels and their first indexes # lab, linds = np.unique(inds, return_index=True) # compute values by indexes # values = data.reshape(-1)[linds] # alternative slow implementation # if there are different data on same index, it will take # maximal value # lab = np.unique(inds) # values = [0]*len(lab) # for label in lab: # values[label] = np.max(data[inds == label]) # # values = np.asarray(values) # yet another implementation values = [None] * (np.max(inds) + 1) linear_inds = inds.ravel() linear_data = data.ravel() for i in range(0, len(linear_inds)): # going over all data pixels if values[linear_inds[i]] is None: # this index is found for first values[linear_inds[i]] = linear_data[i] elif values[linear_inds[i]] < linear_data[i]: # here can be changed maximal or minimal value values[linear_inds[i]] = linear_data[i] values = np.asarray(values) return values\"\ntarget_node_type = \"identifier\"\n\n#encoding \nsource_code_encoding = bert_tokenizer(code)\n\n#masking\nmasked_code_encoding = code_masker(code, bert_tokenizer(code), bert_tokenizer.node_types.index(target_node_type))\n\nassert len(source_code_encoding['input_ids']) == len(masked_code_encoding['input_ids'])\n\n#masked code\nmasked_code = bert_tokenizer.tokenizer.decode(list(filter(lambda token_id: False if token_id == bert_tokenizer.tokenizer.bos_token_id or \n            token_id == bert_tokenizer.tokenizer.eos_token_id else True, masked_code_encoding['input_ids'])))\n\nprint(masked_code)\n\ndef<mask><mask><mask>(<mask>,<mask>):\n    return<mask>*<mask>\n\n\n\n\nCode Prediction\n\nfrom CodeCheckList.predictor import Predictor\n\npredictor = Predictor.from_pretrained(checkpoint, bert_tokenizer)\npredictions = predictor(masked_code_encoding, bert_tokenizer.tokenizer(code, return_tensors='pt'), 5)\n\n\n\nEvaluation\n\nimport CodeCheckList.utils as utils\n\nprediction_number = 0\nprint('------------- CODE -------------')\nprint(code)\nprint('\\n---------- MASKED -------------')\nprint(masked_code)\nprint('\\n--------- PREDICTED -----------')\npredicted_code = predictions[prediction_number]\nprint(predicted_code)\nprint('\\n--------- AST COMPARE -----------')\nfiltered_nodes = []\nfiltered_nodes_predict = []\nutils.find_nodes(bert_tokenizer.parser.parse(bytes(code, \"utf8\")).root_node, bert_tokenizer.node_types[bert_tokenizer.node_types.index(target_node_type)], filtered_nodes)\nutils.find_nodes(bert_tokenizer.parser.parse(bytes(predicted_code, \"utf8\")).root_node, bert_tokenizer.node_types[bert_tokenizer.node_types.index(target_node_type)], filtered_nodes_predict)\nprint(len(filtered_nodes))\nprint(len(filtered_nodes_predict))\n#base the evaluation on size comparison\n\n------------- CODE -------------\ndef multiply_numbers(a,b):\n    return a*b\n\n---------- MASKED -------------\ndef<mask><mask><mask>(<mask>,<mask>):\n    return<mask>*<mask>\n\n--------- PREDICTED -----------\ndef __function(name, value):\n    return f*args\n\n--------- AST COMPARE -----------\n5\n5"
  },
  {
    "objectID": "predictor.html",
    "href": "predictor.html",
    "title": "Predictor",
    "section": "",
    "text": "source\n\nPredictor\n\n Predictor (tokenizer, model)\n\nPredictor Module"
  },
  {
    "objectID": "loader.html",
    "href": "loader.html",
    "title": "CLI MODULE",
    "section": "",
    "text": "source\n\ndownload_grammars\n\n download_grammars (languages)\n\nDownload Tree-sitter grammars\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\nlanguages\nlanguages: Param(“Languages to download”, str, nargs=“+”) = “all”,"
  },
  {
    "objectID": "masker.html",
    "href": "masker.html",
    "title": "Masker Module",
    "section": "",
    "text": "source\n\nMasker\n\n Masker (code_tokenizer:CodeCheckList.tokenizer.CodeTokenizer)\n\nMasker module"
  },
  {
    "objectID": "tokenizer.html",
    "href": "tokenizer.html",
    "title": "CODE MODULE",
    "section": "",
    "text": "source\n\nget_token_type\n\n get_token_type (tok_span:tuple, nodes:list, lines:list)\n\nGet the parent AST type and token AST type of a token.\n\n\n\n\nType\nDetails\n\n\n\n\ntok_span\ntuple\n(start, end) position of a token in tokenizer\n\n\nnodes\nlist\nlist of tree-sitter nodes\n\n\nlines\nlist\nlist of lines in the code\n\n\nReturns\ntuple\n(parent_type, token_type) of the token\n\n\n\n\nsource\n\n\nCodeTokenizer\n\n CodeTokenizer (tokenizer, parser, node_types)\n\nA tokenizer for code, which aligns the tokens with the AST nodes.\n\n# test the tokenizer\npy_tokenizer = CodeTokenizer.from_pretrained(\"gpt2\", \"python\")\ncode = \"def foo():\\n    print('hello world')\"\n\nencoding = py_tokenizer(code)\n\nassert \"ast_ids\" in encoding\nassert \"parent_ast_ids\" in encoding\nassert len(encoding[\"ast_ids\"]) == len(encoding[\"input_ids\"])\nassert len(encoding[\"parent_ast_ids\"]) == len(encoding[\"input_ids\"])\n\nprint(encoding)\nprint(py_tokenizer.tokenizer.convert_ids_to_tokens(encoding[\"input_ids\"]))\n\n{'input_ids': [4299, 22944, 33529, 198, 220, 220, 220, 3601, 10786, 31373, 995, 11537], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 3), (3, 7), (7, 10), (10, 11), (11, 12), (12, 13), (13, 14), (14, 20), (20, 22), (22, 27), (27, 33), (33, 35)], 'ast_ids': [136, 99, 10, -1, -1, -1, -1, 99, 10, 89, 89, 89], 'parent_ast_ids': [69, 69, 115, -1, -1, -1, -1, 114, 101, 101, 101, 101]}\n['def', 'Ġfoo', '():', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġprint', \"('\", 'hello', 'Ġworld', \"')\"]"
  },
  {
    "objectID": "utils.html",
    "href": "utils.html",
    "title": "Utility Methods",
    "section": "",
    "text": "source\n\ntraverse\n\n traverse (node, results)\n\nTraverse in a recursive way, a tree-sitter node and append results to a list.\n\n\n\n\nType\nDetails\n\n\n\n\nnode\n\ntree-sitter node\n\n\nresults\n\nlist to append results to\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nfind_nodes\n\n find_nodes (node, target_node_type, results)\n\nTraverses the tree and find the specified node type\n\n\n\n\nType\nDetails\n\n\n\n\nnode\n\nTree sitter ast treee\n\n\ntarget_node_type\n\nTarget node type to search in the tree\n\n\nresults\n\nList to append the resutls to\n\n\nReturns\nNone\n\n\n\n\n\nsource\n\n\nunroll_node_types\n\n unroll_node_types (nested_node_types:dict)\n\nUnroll nested node types into a flat list of node types. This includes subtypes as well.\n\n\n\n\nType\nDetails\n\n\n\n\nnested_node_types\ndict\nnode_types from tree-sitter\n\n\nReturns\nlist\nlist of node types\n\n\n\n\nsource\n\n\nconvert_to_offset\n\n convert_to_offset (point, lines:list)\n\nConvert the point to an offset\n\n\n\n\nType\nDetails\n\n\n\n\npoint\n\npoint to convert\n\n\nlines\nlist\nlist of lines in the source code"
  }
]